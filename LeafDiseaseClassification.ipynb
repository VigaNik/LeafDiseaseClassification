{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmcSwIpn19T8"
      },
      "source": [
        "# Clean cache (data from earlier runs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iL_TeGq31TRG",
        "outputId": "651fb93e-7ae9-483e-990d-389f1d0eb167"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Removed (if existed): /content/checkpoints\n",
            "[OK] Removed (if existed): /content/logs\n",
            "[OK] Removed (if existed): /content/models\n",
            "[OK] Removed (if existed): /content/runs\n",
            "All old model files cleared. Training will start from scratch.\n"
          ]
        }
      ],
      "source": [
        "# === Clean all old model artifacts ===\n",
        "# This will remove checkpoints, logs, and any saved weights\n",
        "# so that training starts from scratch.\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Common folders where models/logs are usually stored\n",
        "folders_to_remove = [\n",
        "    \"/content/checkpoints\",   # custom checkpoints\n",
        "    \"/content/logs\",          # training logs (tensorboard, etc.)\n",
        "    \"/content/models\",        # saved models\n",
        "    \"/content/runs\",          # often used by torch or tensorboard\n",
        "]\n",
        "for folder in folders_to_remove:\n",
        "    shutil.rmtree(folder, ignore_errors=True)\n",
        "    print(f\"[OK] Removed (if existed): {folder}\")\n",
        "\n",
        "# If you save weights with specific extensions (like .pth, .pt, .h5) in /content\n",
        "# you can also wipe them all:\n",
        "for f in os.listdir(\"/content\"):\n",
        "    if f.endswith((\".pth\", \".pt\", \".h5\", \".ckpt\")):\n",
        "        try:\n",
        "            os.remove(os.path.join(\"/content\", f))\n",
        "            print(f\"[OK] Removed file: {f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not remove {f}: {e}\")\n",
        "\n",
        "print(\"All old model files cleared. Training will start from scratch.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmCZeYj211qX",
        "outputId": "6c20b717-f731-4cb4-e886-6856058de688"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Removed: /content/kagglehub_cache_1758281629\n",
            "[OK] Removed: /content/kagglehub_cache_1758281860\n",
            "[OK] Removed: /content/leaf_disease_data\n",
            "[OK] Removed: /content/sample_data\n",
            " Workspace cleaned. Now you have a fresh Colab environment (except Google Drive).\n"
          ]
        }
      ],
      "source": [
        "# === Full cleanup of Colab working dirs ===\n",
        "import shutil\n",
        "\n",
        "folders_to_remove = [\n",
        "    \"/content/kagglehub_cache_1758281629\",\n",
        "    \"/content/kagglehub_cache_1758281860\",\n",
        "    \"/content/leaf_disease_data\",\n",
        "    \"/content/sample_data\"\n",
        "]\n",
        "\n",
        "for folder in folders_to_remove:\n",
        "    shutil.rmtree(folder, ignore_errors=True)\n",
        "    print(f\"[OK] Removed: {folder}\")\n",
        "\n",
        "print(\" Workspace cleaned. Now you have a fresh Colab environment (except Google Drive).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6J-D-RdsOM6",
        "outputId": "3fee5f9f-cf16-4c16-a787-0f02bda92781"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Removed local cache: /root/.cache/kagglehub\n",
            "[OK] Removed Drive dataset folder (if existed): /content/drive/MyDrive/leaf-disease-dataset-combination\n"
          ]
        }
      ],
      "source": [
        "# === Purge local Colab cache and (optionally) a dataset folder on Google Drive ===\n",
        "# Run this BEFORE any import of kagglehub.\n",
        "\n",
        "import os, shutil\n",
        "\n",
        "# 1) Remove local kagglehub cache inside the Colab VM\n",
        "local_cache = os.path.expanduser(\"~/.cache/kagglehub\")\n",
        "shutil.rmtree(local_cache, ignore_errors=True)\n",
        "print(f\"[OK] Removed local cache: {local_cache}\")\n",
        "\n",
        "# 2) (Optional) If you stored a copy on Google Drive, remove that too\n",
        "#    Change this path if your dataset sits elsewhere in Drive.\n",
        "drive_dataset_dir = \"/content/drive/MyDrive/leaf-disease-dataset-combination\"\n",
        "shutil.rmtree(drive_dataset_dir, ignore_errors=True)\n",
        "print(f\"[OK] Removed Drive dataset folder (if existed): {drive_dataset_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2eVdjwlvRnG",
        "outputId": "cbbea4c0-0f7e-4aad-a3d9-cfc1949fbe58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Using isolated cache dir: /content/kagglehub_cache_1759049045\n",
            "Using Colab cache for faster access to the 'leaf-disease-dataset-combination' dataset.\n",
            "[OK] Fresh dataset downloaded to: /kaggle/input/leaf-disease-dataset-combination\n",
            "[CHECK] Cache root exists: False\n",
            "[CHECK] Cache dir content entries: 0\n"
          ]
        }
      ],
      "source": [
        "# === Force a fresh download with an isolated, empty cache dir ===\n",
        "# IMPORTANT: set env vars BEFORE importing kagglehub.\n",
        "\n",
        "import os, time\n",
        "from pathlib import Path\n",
        "\n",
        "# Put kagglehub cache into a fresh, run-specific folder (so nothing is reused)\n",
        "run_cache = Path(f\"/content/kagglehub_cache_{int(time.time())}\")\n",
        "os.environ[\"XDG_CACHE_HOME\"] = str(run_cache)   # kagglehub honors ~/.cache via XDG_CACHE_HOME\n",
        "print(\"[INFO] Using isolated cache dir:\", run_cache)\n",
        "\n",
        "import kagglehub   # import AFTER setting XDG_CACHE_HOME\n",
        "\n",
        "DATASET_ID = \"asheniranga/leaf-disease-dataset-combination\"\n",
        "\n",
        "# Force download to bypass any previous cache\n",
        "data_path = kagglehub.dataset_download(DATASET_ID, force_download=True)\n",
        "\n",
        "print(\"[OK] Fresh dataset downloaded to:\", data_path)\n",
        "print(\"[CHECK] Cache root exists:\", run_cache.exists())\n",
        "print(\"[CHECK] Cache dir content entries:\", len(list(run_cache.rglob('*'))))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrRDdc69-esf"
      },
      "source": [
        "# Import data, and GPU setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WZ7Qudt-d9u",
        "outputId": "f3d3ad0d-f40f-43fa-dfd1-739b000f96a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            " GPU detected and ready!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# Check GPU availability\n",
        "import tensorflow as tf\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    print(\" GPU detected and ready!\")\n",
        "else:\n",
        "    print(\" Using CPU\")\n",
        "\n",
        "# Simple setup\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"Set2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBCAd_xm5Z9e"
      },
      "source": [
        "\n",
        "\n",
        "> Libraries and data import\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noOivRV1BMvz"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn import __version__ as skver\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, log_loss\n",
        "from sklearn.preprocessing import Normalizer, LabelEncoder\n",
        "from sklearn import metrics\n",
        "import random, glob, os\n",
        "from pathlib import Path\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "import kagglehub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tj0qjLDs5UyW"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter, defaultdict\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Setting up for beautiful graphs\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "1M3epOuWpTQS",
        "outputId": "3ff94313-0cdd-4b5c-8f7b-1943087bf25a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1275214815.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "zip_path = \"/content/drive/MyDrive/ColabNotebooks/LeavesPhotoDF/leaf_disease.zip\"\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "extract_path = \"/content/leaf_disease_data\"\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "data_path = Path(extract_path)\n",
        "\n",
        "print(\"Data path set to:\", data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuerdzP0q83J"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Download latest version\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "data = kagglehub.dataset_download(\"asheniranga/leaf-disease-dataset-combination\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhbwrcKt7k1P"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# === Ensure writable dataset root for duplicate removal ===\n",
        "# If data points to a read-only location (e.g., /kaggle/input),\n",
        "# make a working copy under /content and return a writable root.\n",
        "\n",
        "import os, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "def ensure_writable_root(data: str, work_dir=\"/content/leaf_disease_data\"):\n",
        "    src = Path(data)\n",
        "\n",
        "    # Try to detect the actual dataset root (some datasets have \"image data\" subfolder)\n",
        "    candidates = [src / \"image data\", src]\n",
        "    src_root = next((c for c in candidates if c.exists()), None)\n",
        "    if src_root is None:\n",
        "        raise FileNotFoundError(f\"Dataset path not found: {data}\")\n",
        "\n",
        "    # If we can write here, use it as-is\n",
        "    if os.access(src_root, os.W_OK):\n",
        "        print(\"[OK] Writable dataset root:\", src_root)\n",
        "        return src_root\n",
        "\n",
        "    # Otherwise copy to /content (writable)\n",
        "    dst = Path(work_dir)\n",
        "    shutil.rmtree(dst, ignore_errors=True)\n",
        "    shutil.copytree(src, dst)\n",
        "    print(\"[INFO] Source was read-only. Working copy created at:\", dst)\n",
        "\n",
        "    # Return the same structure (prefer \"image data\" if exists)\n",
        "    dst_root = dst / \"image data\" if (dst / \"image data\").exists() else dst\n",
        "    print(\"[OK] Writable dataset root:\", dst_root)\n",
        "    return dst_root\n",
        "\n",
        "# --- Use it BEFORE your duplicate code ---\n",
        "# data must already be defined (from kagglehub or elsewhere)\n",
        "data_path = ensure_writable_root(data)  # <-- this sets 'root' for your code\n",
        "\n",
        "\n",
        "root = Path(data_path) / \"image data\" if (Path(data_path) / \"image data\").exists() else Path(data_path)\n",
        "print(f\"Dataset root: {root}\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vjp8FCiB7vAQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cl-4F_3pvX8-"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# === Sanity check: show creation/mtime of a few files to confirm a fresh copy ===\n",
        "from pathlib import Path\n",
        "import datetime, os\n",
        "\n",
        "root = Path(data_path)\n",
        "samples = [p for p in root.rglob(\"*\") if p.is_file()][:5]\n",
        "\n",
        "def fmt_ts(ts):\n",
        "    return datetime.datetime.fromtimestamp(ts).isoformat(sep=\" \", timespec=\"seconds\")\n",
        "\n",
        "for p in samples:\n",
        "    st = p.stat()\n",
        "    print(f\"{p.name:40s} | ctime={fmt_ts(st.st_ctime)} | mtime={fmt_ts(st.st_mtime)}\")\n",
        "    # If ctime/mtime look like 'now', it indicates a fresh extraction on this run.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f47Y5UtJ0bEP"
      },
      "source": [
        "#ANALYSIS OF DATASET STRUCTURE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvGnKVdFuzpo"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "root = Path(data_path)\n",
        "if (root / \"image data\").exists():\n",
        "    root = root / \"image data\"\n",
        "\n",
        "EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
        "\n",
        "def walk(p: Path, pref=\"\"):\n",
        "    files = [f for f in p.iterdir() if f.is_file() and f.suffix.lower() in EXTS]\n",
        "    dirs  = [d for d in p.iterdir() if d.is_dir()]\n",
        "    if files:\n",
        "        print(f\"{pref}{p.name}  [images: {len(files)}]\")\n",
        "        return\n",
        "    print(f\"{pref}{p.name}/\")\n",
        "    for d in sorted(dirs, key=lambda x: x.name.lower()):\n",
        "        walk(d, pref + \"  - \")\n",
        "walk(root)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8TeJSXx-9IW"
      },
      "outputs": [],
      "source": [
        "import random, glob, os\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "root = Path(data_path) / \"image data\" if (Path(data_path) / \"image data\").exists() else Path(data_path)\n",
        "exts = (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.bmp\",\"*.webp\")\n",
        "imgs = sum((list(root.rglob(e)) for e in exts), [])\n",
        "sampled = random.sample(imgs, 8)\n",
        "\n",
        "fig, ax = plt.subplots(2, 4, figsize=(8,4))\n",
        "for i in range(2):\n",
        "    for j in range(4):\n",
        "        p = sampled[i*4 + j]\n",
        "        img = Image.open(p)\n",
        "        ax[i, j].imshow(img)\n",
        "        rel = p.relative_to(root)\n",
        "        ax[i, j].set_title(\"/\".join(rel.parts[-3:-1]), fontsize=8)  # last two folders\n",
        "        ax[i, j].axis('off')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIKLVxyvCdxH"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "root = Path(data_path) / \"image data\" if (Path(data_path) / \"image data\").exists() else Path(data_path)\n",
        "\n",
        "for split in ['train', 'test', 'validation']:\n",
        "    split_path = root / split\n",
        "    if split_path.exists():\n",
        "        for plant in os.listdir(split_path):\n",
        "            plant_path = split_path / plant\n",
        "            if plant_path.is_dir():\n",
        "                for disease in os.listdir(plant_path):\n",
        "                    disease_path = plant_path / disease\n",
        "                    if disease_path.is_dir():\n",
        "                        image_count = len([f for f in os.listdir(disease_path) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.webp'))])\n",
        "                        is_healthy = 'healthy' in disease.lower() # Assuming 'healthy' is in the folder name for healthy plants\n",
        "                        data.append([split, plant, disease, image_count, is_healthy])\n",
        "\n",
        "df = pd.DataFrame(data, columns=['split', 'plant', 'disease', 'count', 'is_healthy'])\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyTnrXgALeQB"
      },
      "source": [
        "##Visualization  by Hierarchical Clusteringera and Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX1fVcXlK5AW"
      },
      "source": [
        "\n",
        "\n",
        "> 'Plant Similarity Matrix and Disease Co-occurrence Matrix\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgaHnFNhDAFp"
      },
      "outputs": [],
      "source": [
        "def simple_clustering_viz(df):\n",
        "    \"\"\"Simple clustering using GPU when possible\"\"\"\n",
        "\n",
        "    # Create simple feature matrix\n",
        "    pivot_table = df.pivot_table(values='count',\n",
        "                                index='plant',\n",
        "                                columns='disease',\n",
        "                                fill_value=0)\n",
        "\n",
        "    # Use GPU for computation if available\n",
        "    if tf.config.list_physical_devices('GPU'):\n",
        "        with tf.device('/GPU:0'):\n",
        "            # Convert to tensor for GPU computation\n",
        "            data_tensor = tf.constant(pivot_table.values, dtype=tf.float32)\n",
        "\n",
        "            # Simple distance calculation on GPU\n",
        "            distances = tf.reduce_sum(tf.square(data_tensor[:, None] - data_tensor[None, :]), axis=2)\n",
        "            distances_np = distances.numpy()\n",
        "    else:\n",
        "        # CPU fallback\n",
        "        from scipy.spatial.distance import pdist, squareform\n",
        "        distances_np = squareform(pdist(pivot_table.values))\n",
        "\n",
        "    # Simple hierarchical clustering visualization\n",
        "    plt.figure(figsize=(22, 10))\n",
        "\n",
        "    # Create a simple similarity heatmap\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.heatmap(distances_np,\n",
        "                xticklabels=pivot_table.index,\n",
        "                yticklabels=pivot_table.index,\n",
        "                cmap='viridis_r',\n",
        "                cbar_kws={'label': 'Distance'})\n",
        "    plt.title('Plant Similarity Matrix')\n",
        "    plt.xlabel('Plants')\n",
        "    plt.ylabel('Plants')\n",
        "\n",
        "\n",
        "\n",
        "     # Simple hierarchical clustering visualization\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    # Disease co-occurrence matrix\n",
        "    plt.subplot(1, 2, 2)\n",
        "    disease_cooccur = (pivot_table > 0).astype(int)\n",
        "    sns.heatmap(disease_cooccur.T @ disease_cooccur,\n",
        "                cmap='Blues',\n",
        "                cbar_kws={'label': 'Co-occurrence'})\n",
        "    plt.title('Disease Co-occurrence Matrix')\n",
        "    plt.xlabel('Diseases')\n",
        "    plt.ylabel('Diseases')\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "simple_clustering_viz(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGvazpYTLO9I"
      },
      "source": [
        "\n",
        "\n",
        "> Plant-Disease Distribution\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOEL-WsUJVo4"
      },
      "outputs": [],
      "source": [
        "def plot_plant_disease_matrix(df):\n",
        "    \"\"\"Create a comprehensive plant-disease matrix showing all relationships\"\"\"\n",
        "\n",
        "    print(f\"\\n🔍 PLANT-DISEASE RELATIONSHIP MATRIX\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Create pivot table\n",
        "    pivot_table = df.pivot_table(values='count',\n",
        "                                index='plant',\n",
        "                                columns='disease',\n",
        "                                fill_value=0)\n",
        "\n",
        "    # Create the heatmap\n",
        "    plt.figure(figsize=(20, 12))\n",
        "\n",
        "    # Use log scale for better visualization\n",
        "    pivot_log = np.log1p(pivot_table)  # log(1+x) to handle zeros\n",
        "\n",
        "    # Create heatmap\n",
        "    sns.heatmap(pivot_log,\n",
        "                cmap='YlOrRd',\n",
        "                cbar_kws={'label': 'Log(Image Count + 1)'},\n",
        "                linewidths=0.5,\n",
        "                square=False)\n",
        "\n",
        "    plt.title('Plant-Disease Distribution Matrix (All Plants & Diseases)\\nLog scale for better visibility',\n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "    plt.xlabel('Diseases', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('Plants', fontsize=12, fontweight='bold')\n",
        "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "    plt.yticks(rotation=0, fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Show top combinations per plant\n",
        "    print(f\"\\n TOP DISEASE PER PLANT:\")\n",
        "    for plant in pivot_table.index:\n",
        "        plant_row = pivot_table.loc[plant]\n",
        "        top_disease = plant_row.idxmax()\n",
        "        top_count = plant_row.max()\n",
        "        print(f\"   {plant:<25}: {top_disease:<30} ({top_count:,} images)\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "plot_plant_disease_matrix(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1scQwhQKrpl"
      },
      "source": [
        "> Analysis: Top Plant-Disease Combinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKtRM0fVYzcr"
      },
      "outputs": [],
      "source": [
        "def plot_complete_hierarchy(df):\n",
        "    \"\"\"Create comprehensive hierarchical plots showing all plants and diseases\"\"\"\n",
        "\n",
        "    # Create larger figure to accommodate all data\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
        "\n",
        "    # 1. Split distribution\n",
        "    split_counts = df.groupby('split')['count'].sum()\n",
        "    colors_splits = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
        "    axes[0,0].pie(split_counts.values, labels=split_counts.index, autopct='%1.1f%%',\n",
        "                  colors=colors_splits, startangle=90)\n",
        "    axes[0,0].set_title('Distribution by Split', fontsize=14, fontweight='bold')\n",
        "\n",
        "    # 2. ALL Plants distribution (horizontal bar chart for better readability)\n",
        "    plant_counts = df.groupby('plant')['count'].sum().sort_values(ascending=True)\n",
        "\n",
        "    # Create color map for plants\n",
        "    colors_plants = plt.cm.Set3(np.linspace(0, 1, len(plant_counts)))\n",
        "\n",
        "    bars = axes[0,1].barh(range(len(plant_counts)), plant_counts.values, color=colors_plants)\n",
        "    axes[0,1].set_yticks(range(len(plant_counts)))\n",
        "    axes[0,1].set_yticklabels(plant_counts.index, fontsize=10)\n",
        "    axes[0,1].set_title('Images per Plant (All 13 Plants)', fontsize=14, fontweight='bold')\n",
        "    axes[0,1].set_xlabel('Number of Images')\n",
        "    axes[0,1].grid(axis='x', alpha=0.3)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for i, bar in enumerate(bars):\n",
        "        width = bar.get_width()\n",
        "        axes[0,1].text(width + width*0.01, bar.get_y() + bar.get_height()/2,\n",
        "                      f'{int(width):,}', ha='left', va='center', fontsize=9)\n",
        "\n",
        "    # 3. Top diseases distribution (show more diseases)\n",
        "    disease_counts = df.groupby('disease')['count'].sum().sort_values(ascending=False).head(15)\n",
        "\n",
        "    # Rotate the bar chart and make it more readable\n",
        "    disease_counts.plot(kind='bar', ax=axes[1,0], color='coral', alpha=0.8)\n",
        "    axes[1,0].set_title('Top 15 Diseases by Image Count', fontsize=14, fontweight='bold')\n",
        "    axes[1,0].set_xlabel('Disease')\n",
        "    axes[1,0].set_ylabel('Number of Images')\n",
        "    axes[1,0].tick_params(axis='x', rotation=45, labelsize=8)\n",
        "    axes[1,0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add value labels on top of bars\n",
        "    for i, (disease, count) in enumerate(disease_counts.items()):\n",
        "        axes[1,0].text(i, count + count*0.01, f'{int(count):,}',\n",
        "                      ha='center', va='bottom', fontsize=8, rotation=0)\n",
        "\n",
        "    # 4. Healthy vs Diseased\n",
        "    health_counts = df.groupby('is_healthy')['count'].sum()\n",
        "    colors_health = ['#FF4444', '#44FF44']  # Red for diseased, green for healthy\n",
        "    labels_health = ['Diseased', 'Healthy']\n",
        "\n",
        "    wedges, texts, autotexts = axes[1,1].pie(health_counts.values,\n",
        "                                           labels=labels_health,\n",
        "                                           autopct='%1.1f%%',\n",
        "                                           colors=colors_health,\n",
        "                                           startangle=90,\n",
        "                                           explode=(0.05, 0))  # Slightly separate slices\n",
        "\n",
        "    axes[1,1].set_title('Healthy vs Diseased Distribution', fontsize=14, fontweight='bold')\n",
        "\n",
        "    # Make percentage text more readable\n",
        "    for autotext in autotexts:\n",
        "        autotext.set_color('white')\n",
        "        autotext.set_fontweight('bold')\n",
        "        autotext.set_fontsize(12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Additional detailed breakdown\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" COMPLETE PLANT BREAKDOWN (All 13 Plants)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    total_images = df['count'].sum()\n",
        "\n",
        "    for i, (plant, count) in enumerate(plant_counts.sort_values(ascending=False).items(), 1):\n",
        "        percentage = (count / total_images) * 100\n",
        "\n",
        "        # Get disease count for this plant\n",
        "        plant_diseases = df[df['plant'] == plant]['disease'].nunique()\n",
        "\n",
        "        # Get healthy percentage for this plant\n",
        "        plant_data = df[df['plant'] == plant]\n",
        "        healthy_count = plant_data[plant_data['is_healthy'] == True]['count'].sum()\n",
        "        healthy_pct = (healthy_count / count) * 100 if count > 0 else 0\n",
        "\n",
        "        print(f\"{i:2d}. {plant:<25} : {count:>6,} images ({percentage:5.1f}%) | \"\n",
        "              f\"{plant_diseases:2d} diseases | {healthy_pct:4.1f}% healthy\")\n",
        "\n",
        "    print(f\"\\n SUMMARY STATISTICS:\")\n",
        "    print(f\"   Total Plants: {plant_counts.nunique()}\")\n",
        "    print(f\"   Total Images: {total_images:,}\")\n",
        "    print(f\"   Average per Plant: {total_images/len(plant_counts):,.0f} images\")\n",
        "    print(f\"   Largest Plant: {plant_counts.max():,} images ({plant_counts.idxmax()})\")\n",
        "    print(f\"   Smallest Plant: {plant_counts.min():,} images ({plant_counts.idxmin()})\")\n",
        "    print(f\"   Imbalance Ratio: {plant_counts.max()/plant_counts.min():.1f}x\")\n",
        "\n",
        "# Execute both functions\n",
        "print(\"Creating complete hierarchy visualization...\")\n",
        "plot_complete_hierarchy(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCcVNK_ERjkx"
      },
      "source": [
        "#Data cleaning and preperation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1epPvoygJ8_"
      },
      "source": [
        "Heare we cleaning \"orange\" and \"squash\" leaves. Because we don't have healthy seples into. That resone why we disaide to frop this failes. also data is very anbalanse. We have to much tomato and for exthample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FvQXW5_5gEp7"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import shutil\n",
        "import re\n",
        "import os\n",
        "\n",
        "# dataset root (after unzip)\n",
        "root = Path(data_path)\n",
        "\n",
        "DROP_RAW = {\"orange\", \"squash\"}\n",
        "\n",
        "def norm_name(s: str) -> str:\n",
        "    s = s.lower().strip()\n",
        "    s = s.replace(\"___\", \" \").replace(\"__\", \" \").replace(\"_\", \" \")\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s\n",
        "\n",
        "DROP = {norm_name(x) for x in DROP_RAW}\n",
        "\n",
        "removed = 0\n",
        "for root, dirs, files in os.walk(root):\n",
        "    for dname in list(dirs):\n",
        "        if norm_name(dname) in DROP:\n",
        "            p = Path(root) / dname\n",
        "            shutil.rmtree(p, ignore_errors=True)\n",
        "            removed += 1\n",
        "            print(f\"Removed: {p}\")\n",
        "\n",
        "print(\"Total removed dirs:\", removed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlvcG5V0a2sR"
      },
      "outputs": [],
      "source": [
        "!pip -q install imagehash"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxCzeBYad1hi"
      },
      "source": [
        "Likely issues to check\n",
        "\n",
        "Corrupt/unreadable files, wrong extensions\n",
        "\n",
        "Class imbalance and long-tail classes\n",
        "\n",
        "Inconsistent labels (case/spaces/punctuation)\n",
        "\n",
        "Leakage/duplicates across train/val/test\n",
        "\n",
        "Wildly varying sizes/aspect ratios; non-RGB/alpha channels; EXIF rotation\n",
        "\n",
        "Very small/blank images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uh58ntnZavQw"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from PIL import Image, ImageOps\n",
        "import imagehash, numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "import re, random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "root = Path(data_path)\n",
        "root = root / \"image data\" if (root / \"image data\").exists() else root\n",
        "EXTS = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\"}\n",
        "SPLITS = [p.name for p in root.iterdir() if p.is_dir()]  # auto-detect splits\n",
        "SPLITS = sorted([s for s in SPLITS if (root/s).is_dir()])\n",
        "\n",
        "def norm_label(s):\n",
        "    return re.sub(r\"[^a-z0-9]+\",\"_\", s.lower()).strip(\"_\")\n",
        "\n",
        "tot, bad = 0, []\n",
        "dims_w, dims_h = [], []\n",
        "modes = Counter()\n",
        "per_split = Counter()\n",
        "per_class = Counter()\n",
        "per_split_class = Counter()\n",
        "label_map = defaultdict(set)   # normalized -> original variants\n",
        "hash_to_paths = defaultdict(list)\n",
        "\n",
        "# Limit hashing if huge (for speed). Set to None to hash all.\n",
        "MAX_HASH = 6000\n",
        "hash_sample = []\n",
        "\n",
        "for split in SPLITS:\n",
        "    split_dir = root / split\n",
        "    for cls in sorted([d.name for d in split_dir.iterdir() if d.is_dir()]):\n",
        "        cls_dir = split_dir / cls\n",
        "        label_map[norm_label(cls)].add(cls)\n",
        "        for p in cls_dir.rglob(\"*\"):\n",
        "            if p.suffix.lower() not in EXTS: continue\n",
        "            try:\n",
        "                with Image.open(p) as im:\n",
        "                    im = ImageOps.exif_transpose(im)\n",
        "                    modes[im.mode] += 1\n",
        "                    w,h = im.size\n",
        "                    dims_w.append(w); dims_h.append(h)\n",
        "            except Exception as e:\n",
        "                bad.append((str(p), str(e))); continue\n",
        "            tot += 1\n",
        "            per_split[split] += 1\n",
        "            per_class[cls] += 1\n",
        "            per_split_class[(split, cls)] += 1\n",
        "            # sample for duplicate/leakage check\n",
        "            if MAX_HASH is None or len(hash_sample) < MAX_HASH or random.random() < MAX_HASH/max(1,tot):\n",
        "                hash_sample.append(p)\n",
        "\n",
        "# compute perceptual hashes on sample\n",
        "for p in hash_sample:\n",
        "    try:\n",
        "        with Image.open(p) as im:\n",
        "            ah = imagehash.phash(im, hash_size=12)  # robust & reasonably fast\n",
        "        hash_to_paths[str(ah)].append(str(p))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# summarize\n",
        "def q(x, qs=(.01,.5,.99)):\n",
        "    return tuple(int(np.quantile(x, q)) for q in qs)\n",
        "\n",
        "print(\"=== Basic counts ===\")\n",
        "print(\"Total images:\", tot)\n",
        "for s in SPLITS: print(f\"  {s}: {per_split[s]}\")\n",
        "\n",
        "print(\"\\n=== Classes (Top 5 most/least) ===\")\n",
        "most = per_class.most_common(5)\n",
        "least = sorted(per_class.items(), key=lambda kv: kv[1])[:5]\n",
        "print(\"Most:\", most)\n",
        "print(\"Least:\", least)\n",
        "\n",
        "print(\"\\n=== Image sizes (W,H) quantiles [1%, 50%, 99%] ===\")\n",
        "print(\"W:\", q(dims_w), \"H:\", q(dims_h))\n",
        "\n",
        "print(\"\\n=== Modes (color channels) ===\")\n",
        "print(dict(modes))  # expect mostly 'RGB'; flags if 'L','RGBA', etc.\n",
        "\n",
        "# label inconsistencies\n",
        "collisions = {k:v for k,v in label_map.items() if len(v) > 1}\n",
        "print(\"\\n=== Label normalization collisions (potential inconsistencies) ===\")\n",
        "print(collisions if collisions else \"None\")\n",
        "\n",
        "# corrupts\n",
        "print(\"\\n=== Corrupt/unreadable files ===\")\n",
        "print(f\"Count: {len(bad)}\")\n",
        "for p, e in bad[:5]: print(\"  \", p, \"|\", e)\n",
        "if len(bad) > 5: print(\"  ...\")\n",
        "\n",
        "# duplicates/leakage (sampled)\n",
        "dupe_groups = {h:ps for h,ps in hash_to_paths.items() if len(ps) > 1}\n",
        "print(\"\\n=== Near-duplicates / cross-split leakage (sampled) ===\")\n",
        "print(f\"Groups with >1 match (sample size {len(hash_sample)}):\", len(dupe_groups))\n",
        "shown = 0\n",
        "for h, paths in list(dupe_groups.items())[:3]:\n",
        "    splits = {Path(p).parts[-len(Path(root).parts)-3] for p in paths if \"image data\" in p}\n",
        "    print(f\"  hash {h} | splits seen: {splits}\")\n",
        "    for p in paths[:4]: print(\"    \", p)\n",
        "    shown += 1\n",
        "if len(dupe_groups) > shown: print(\"  ...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZrdk4As4vkU"
      },
      "source": [
        "##Duplicates detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "L0H1lH9z1fGD"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from PIL import Image, ImageOps\n",
        "import imagehash\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "root = Path(data_path) / \"image data\"\n",
        "exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
        "\n",
        "def get_all_images():\n",
        "    \"\"\"Get all images with metadata\"\"\"\n",
        "    images = []\n",
        "    for split in [\"train\", \"validation\", \"test\"]:\n",
        "        split_dir = root / split\n",
        "        if split_dir.exists():\n",
        "            for img_path in split_dir.rglob(\"*\"):\n",
        "                if img_path.suffix.lower() in exts:\n",
        "                    parts = img_path.relative_to(split_dir).parts\n",
        "                    images.append({\n",
        "                        'path': img_path,\n",
        "                        'split': split,\n",
        "                        'plant': parts[0] if parts else 'unknown',\n",
        "                        'name': img_path.name\n",
        "                    })\n",
        "    return images\n",
        "\n",
        "def compute_hash(img_path):\n",
        "    \"\"\"Compute multiple hashes for better accuracy\"\"\"\n",
        "    try:\n",
        "        with Image.open(img_path) as img:\n",
        "            img = ImageOps.exif_transpose(img).convert(\"RGB\")  # Ensure 3 channels\n",
        "            # Use 3 different hash methods\n",
        "            phash = str(imagehash.phash(img, hash_size=12))\n",
        "            ahash = str(imagehash.average_hash(img, hash_size=12))\n",
        "            dhash = str(imagehash.dhash(img, hash_size=12))\n",
        "            return f\"{phash}_{ahash}_{dhash}\"\n",
        "    except Exception as e:\n",
        "        # print(f\"Error processing {img_path}: {e}\") # Optional: uncomment for debugging\n",
        "        return None\n",
        "\n",
        "def find_duplicates():\n",
        "    \"\"\"Find all duplicates using hash comparison\"\"\"\n",
        "    print(\" Finding duplicates in all images...\")\n",
        "    # Get all images\n",
        "    all_images = get_all_images()\n",
        "    print(f\" Processing {len(all_images):,} images\")\n",
        "\n",
        "    # Compute hashes\n",
        "    hash_to_images = defaultdict(list)\n",
        "    for img_info in tqdm(all_images, desc=\"Computing hashes\"):\n",
        "        img_hash = compute_hash(img_info['path'])\n",
        "        if img_hash:\n",
        "            hash_to_images[img_hash].append(img_info)\n",
        "\n",
        "    # Find duplicate groups\n",
        "    duplicates = [group for group in hash_to_images.values() if len(group) > 1]\n",
        "    print(f\"\\n Results:\")\n",
        "    print(f\" Found {len(duplicates)} duplicate groups\")\n",
        "    print(f\" Total duplicates: {sum(len(group)-1 for group in duplicates)}\")\n",
        "    return duplicates\n",
        "\n",
        "def analyze_duplicates(duplicate_groups):\n",
        "    \"\"\"Analyze and show duplicate patterns\"\"\"\n",
        "    cross_split = 0\n",
        "    same_split = 0\n",
        "    print(f\"\\n Duplicate Analysis:\")\n",
        "    for i, group in enumerate(duplicate_groups[:5]): # Show first 5 groups\n",
        "        splits = set(img['split'] for img in group)\n",
        "        if len(splits) > 1:\n",
        "            cross_split += 1\n",
        "            print(f\"\\n Group {i+1} (CROSS-SPLIT):\")\n",
        "        else:\n",
        "            same_split += 1\n",
        "            print(f\"\\n Group {i+1} (Same split: {list(splits)[0]}):\")\n",
        "        for img in group:\n",
        "            print(f\"  {img['split']}/{img['plant']}/{img['name']}\")\n",
        "    print(f\"\\n Summary:\")\n",
        "    print(f\"  Cross-split duplicates: {cross_split}\")\n",
        "    print(f\"  Same-split duplicates: {same_split}\")\n",
        "    return cross_split > 0\n",
        "\n",
        "def show_examples(duplicate_groups, num_examples=2):\n",
        "    \"\"\"Show visual examples of duplicates\"\"\"\n",
        "    print(f\"\\n Visual Examples:\")\n",
        "    for i, group in enumerate(duplicate_groups[:num_examples]):\n",
        "        fig, axes = plt.subplots(1, min(len(group), 4), figsize=(12, 3))\n",
        "        if len(group) == 1:\n",
        "             axes = [axes] # Ensure axes is iterable even for single image group\n",
        "        for j, img in enumerate(group[:4]):\n",
        "            try:\n",
        "                with Image.open(img['path']) as pil_img:\n",
        "                    axes[j].imshow(pil_img)\n",
        "                    axes[j].set_title(f\"{img['split']}\\n{img['plant']}\", fontsize=9)\n",
        "                    axes[j].axis('off')\n",
        "            except:\n",
        "                axes[j].text(0.5, 0.5, \"Error\", ha='center')\n",
        "        plt.suptitle(f'Duplicate Group {i+1}')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def remove_duplicates(duplicate_groups):\n",
        "    \"\"\"Remove duplicates keeping train > validation > test priority\"\"\"\n",
        "    priority = ['train', 'validation', 'test']\n",
        "    removed = 0\n",
        "    print(f\"\\n Removing duplicates (priority: {' > '.join(priority)}):\")\n",
        "    for group in duplicate_groups:\n",
        "        # Sort by priority\n",
        "        sorted_group = sorted(group, key=lambda x: priority.index(x['split']) if x['split'] in priority else 999)\n",
        "        # Keep first, remove others\n",
        "        keep = sorted_group[0]\n",
        "        remove_list = sorted_group[1:]\n",
        "\n",
        "        # Check if the image to keep actually exists before printing keep message\n",
        "        if keep['path'].exists():\n",
        "            print(f\" Keep: {keep['split']}/{keep['plant']}/{keep['name']}\")\n",
        "        else:\n",
        "             # If the image to keep doesn't exist, keep the next available one\n",
        "             print(f\" Warning: Image to keep {keep['path']} not found. Keeping the next available image in the group.\")\n",
        "             keep = None\n",
        "             for img in sorted_group:\n",
        "                 if img['path'].exists():\n",
        "                     keep = img\n",
        "                     print(f\" Keep: {keep['split']}/{keep['plant']}/{keep['name']}\")\n",
        "                     break\n",
        "             if keep is None:\n",
        "                 print(\" Error: No image to keep found in the group.\")\n",
        "                 continue # Skip to next group if no image to keep is found\n",
        "\n",
        "             remove_list = [img for img in sorted_group if img != keep]\n",
        "\n",
        "\n",
        "        for img in remove_list:\n",
        "            try:\n",
        "                if img['path'].exists(): # Check if file exists before attempting to remove\n",
        "                    img['path'].unlink()\n",
        "                    print(f\" Remove: {img['split']}/{img['plant']}/{img['name']}\")\n",
        "                    removed += 1\n",
        "                # else:\n",
        "                    # print(f\" Info: Image to remove {img['path']} not found (already removed?)\") # Optional: uncomment for debugging\n",
        "            except Exception as e:\n",
        "                print(f\"Error removing {img['path']}: {e}\") # Print specific error\n",
        "\n",
        "    print(f\"\\n Removed {removed} duplicate files\")\n",
        "    return removed\n",
        "\n",
        "# Execute duplicate detection\n",
        "print(\" SIMPLE DUPLICATE DETECTION\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Find duplicates\n",
        "duplicates = find_duplicates()\n",
        "\n",
        "if duplicates:\n",
        "    # Analyze patterns\n",
        "    has_cross_split = analyze_duplicates(duplicates)\n",
        "\n",
        "    # Show examples\n",
        "    show_examples(duplicates)\n",
        "\n",
        "    # Warning for cross-split duplicates\n",
        "    if has_cross_split:\n",
        "        print(f\"\\n WARNING: Cross-split duplicates found!\")\n",
        "        print(f\" This causes data leakage - model sees test data during training\")\n",
        "\n",
        "    # Ask before removing\n",
        "    print(f\"\\n Remove {sum(len(group)-1 for group in duplicates)} duplicate files?\")\n",
        "    print(f\" Uncomment the line below to proceed:\")\n",
        "    # print(f\" removed_count = remove_duplicates(duplicates)\")\n",
        "\n",
        "    # Uncomment to actually remove:\n",
        "    removed_count = remove_duplicates(duplicates)\n",
        "\n",
        "else:\n",
        "    print(\"No duplicates found!\")\n",
        "\n",
        "print(f\"\\n Detection completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7PuhtQ7pt2S"
      },
      "source": [
        "\n",
        "\n",
        "> Visualisation cheking\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZC3G215PdeO5"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "root = Path(data_path) / \"image data\" if (Path(data_path) / \"image data\").exists() else Path(data_path)\n",
        "\n",
        "for split in ['train', 'test', 'validation']:\n",
        "    split_path = root / split\n",
        "    if split_path.exists():\n",
        "        for plant in os.listdir(split_path):\n",
        "            plant_path = split_path / plant\n",
        "            if plant_path.is_dir():\n",
        "                for disease in os.listdir(plant_path):\n",
        "                    disease_path = plant_path / disease\n",
        "                    if disease_path.is_dir():\n",
        "                        image_count = len([f for f in os.listdir(disease_path) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.webp'))])\n",
        "                        is_healthy = 'healthy' in disease.lower() # Assuming 'healthy' is in the folder name for healthy plants\n",
        "                        data.append([split, plant, disease, image_count, is_healthy])\n",
        "\n",
        "df = pd.DataFrame(data, columns=['split', 'plant', 'disease', 'count', 'is_healthy'])\n",
        "display(df.head())\n",
        "\n",
        "def plot_complete_hierarchy(df):\n",
        "    \"\"\"Create comprehensive hierarchical plots showing all plants and diseases\"\"\"\n",
        "\n",
        "    # Create larger figure to accommodate all data\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
        "\n",
        "    # 1. Split distribution\n",
        "    split_counts = df.groupby('split')['count'].sum()\n",
        "    colors_splits = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
        "    axes[0,0].pie(split_counts.values, labels=split_counts.index, autopct='%1.1f%%',\n",
        "                  colors=colors_splits, startangle=90)\n",
        "    axes[0,0].set_title('Distribution by Split', fontsize=14, fontweight='bold')\n",
        "\n",
        "    # 2. ALL Plants distribution (horizontal bar chart for better readability)\n",
        "    plant_counts = df.groupby('plant')['count'].sum().sort_values(ascending=True)\n",
        "\n",
        "    # Create color map for plants\n",
        "    colors_plants = plt.cm.Set3(np.linspace(0, 1, len(plant_counts)))\n",
        "\n",
        "    bars = axes[0,1].barh(range(len(plant_counts)), plant_counts.values, color=colors_plants)\n",
        "    axes[0,1].set_yticks(range(len(plant_counts)))\n",
        "    axes[0,1].set_yticklabels(plant_counts.index, fontsize=10)\n",
        "    axes[0,1].set_title('Images per Plant (All 13 Plants)', fontsize=14, fontweight='bold')\n",
        "    axes[0,1].set_xlabel('Number of Images')\n",
        "    axes[0,1].grid(axis='x', alpha=0.3)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for i, bar in enumerate(bars):\n",
        "        width = bar.get_width()\n",
        "        axes[0,1].text(width + width*0.01, bar.get_y() + bar.get_height()/2,\n",
        "                      f'{int(width):,}', ha='left', va='center', fontsize=9)\n",
        "\n",
        "    # 3. Top diseases distribution (show more diseases)\n",
        "    disease_counts = df.groupby('disease')['count'].sum().sort_values(ascending=False).head(15)\n",
        "\n",
        "    # Rotate the bar chart and make it more readable\n",
        "    disease_counts.plot(kind='bar', ax=axes[1,0], color='coral', alpha=0.8)\n",
        "    axes[1,0].set_title('Top 15 Diseases by Image Count', fontsize=14, fontweight='bold')\n",
        "    axes[1,0].set_xlabel('Disease')\n",
        "    axes[1,0].set_ylabel('Number of Images')\n",
        "    axes[1,0].tick_params(axis='x', rotation=45, labelsize=8)\n",
        "    axes[1,0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add value labels on top of bars\n",
        "    for i, (disease, count) in enumerate(disease_counts.items()):\n",
        "        axes[1,0].text(i, count + count*0.01, f'{int(count):,}',\n",
        "                      ha='center', va='bottom', fontsize=8, rotation=0)\n",
        "\n",
        "    # 4. Healthy vs Diseased\n",
        "    health_counts = df.groupby('is_healthy')['count'].sum()\n",
        "    colors_health = ['#FF4444', '#44FF44']  # Red for diseased, green for healthy\n",
        "    labels_health = ['Diseased', 'Healthy']\n",
        "\n",
        "    wedges, texts, autotexts = axes[1,1].pie(health_counts.values,\n",
        "                                           labels=labels_health,\n",
        "                                           autopct='%1.1f%%',\n",
        "                                           colors=colors_health,\n",
        "                                           startangle=90,\n",
        "                                           explode=(0.05, 0))  # Slightly separate slices\n",
        "\n",
        "    axes[1,1].set_title('Healthy vs Diseased Distribution', fontsize=14, fontweight='bold')\n",
        "\n",
        "    # Make percentage text more readable\n",
        "    for autotext in autotexts:\n",
        "        autotext.set_color('white')\n",
        "        autotext.set_fontweight('bold')\n",
        "        autotext.set_fontsize(12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Additional detailed breakdown\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPLETE PLANT BREAKDOWN (All 13 Plants)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    total_images = df['count'].sum()\n",
        "\n",
        "    for i, (plant, count) in enumerate(plant_counts.sort_values(ascending=False).items(), 1):\n",
        "        percentage = (count / total_images) * 100\n",
        "\n",
        "        # Get disease count for this plant\n",
        "        plant_diseases = df[df['plant'] == plant]['disease'].nunique()\n",
        "\n",
        "        # Get healthy percentage for this plant\n",
        "        plant_data = df[df['plant'] == plant]\n",
        "        healthy_count = plant_data[plant_data['is_healthy'] == True]['count'].sum()\n",
        "        healthy_pct = (healthy_count / count) * 100 if count > 0 else 0\n",
        "\n",
        "        print(f\"{i:2d}. {plant:<25} : {count:>6,} images ({percentage:5.1f}%) | \"\n",
        "              f\"{plant_diseases:2d} diseases | {healthy_pct:4.1f}% healthy\")\n",
        "\n",
        "    print(f\"\\n SUMMARY STATISTICS:\")\n",
        "    print(f\"   Total Plants: {plant_counts.nunique()}\")\n",
        "    print(f\"   Total Images: {total_images:,}\")\n",
        "    print(f\"   Average per Plant: {total_images/len(plant_counts):,.0f} images\")\n",
        "    print(f\"   Largest Plant: {plant_counts.max():,} images ({plant_counts.idxmax()})\")\n",
        "    print(f\"   Smallest Plant: {plant_counts.min():,} images ({plant_counts.idxmin()})\")\n",
        "    print(f\"   Imbalance Ratio: {plant_counts.max()/plant_counts.min():.1f}x\")\n",
        "\n",
        "# Execute both functions\n",
        "print(\"Creating complete hierarchy visualization...\")\n",
        "plot_complete_hierarchy(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwWgXDhI4P3Z"
      },
      "source": [
        "##Normalization and Spleat data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywYLnEv5u7_C"
      },
      "source": [
        "\n",
        "\n",
        ">  Individul Standartitation Normalization (This type of normalithation normalizede itch photo by individual harateristik with average value of each image: 0 and range [-6,6]. It help stantarizate iach photo to same spector characteristik. That will help crearly see spcific paterns on photo and recocnize them after)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACExrQiw6uip"
      },
      "source": [
        "In this cell we reorder train/validation/test proporthionaly for each folder  with photo for exthemle:\n",
        "-test\n",
        "--plant name\n",
        "---disease name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8Zs7e9Kpwz3"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os, shutil\n",
        "import numpy as np\n",
        "\n",
        "# If there is \"image data\" folder, use it; otherwise use root\n",
        "ROOT = Path(data_path) / \"image data\" if (Path(data_path) / \"image data\").exists() else Path(data_path)\n",
        "\n",
        "# Fractions for splits (must sum to 1.0)\n",
        "TRAIN_FRAC, VAL_FRAC, TEST_FRAC = 0.80, 0.10, 0.10\n",
        "assert abs(TRAIN_FRAC + VAL_FRAC + TEST_FRAC - 1.0) < 1e-6\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "rng = np.random.default_rng(RANDOM_SEED)\n",
        "\n",
        "# Allowed image extensions\n",
        "EXTS = {'.jpg', '.jpeg', '.png', '.bmp', '.webp', '.tif', '.tiff'}\n",
        "\n",
        "# Source split folders we will read from (do NOT delete them until the end)\n",
        "SOURCE_SPLITS = [\"train\", \"validation\", \"val\", \"dev\", \"test\", \"testing\"]\n",
        "\n",
        "# Staging root to build new splits safely\n",
        "STAGING = ROOT / \"_rebuilt_splits\"\n",
        "\n",
        "def safe_rmtree(p: Path):\n",
        "    if p.exists():\n",
        "        shutil.rmtree(p)\n",
        "\n",
        "# 1) Collect all images from any existing split (without modifying current folders)\n",
        "groups = {}  # key=(plant, disease) -> list of file paths\n",
        "\n",
        "def add_image(plant, disease, img_path):\n",
        "    groups.setdefault((plant, disease), []).append(img_path)\n",
        "\n",
        "for split in SOURCE_SPLITS:\n",
        "    split_dir = ROOT / split\n",
        "    if not split_dir.exists():\n",
        "        continue\n",
        "    for plant_dir in split_dir.iterdir():\n",
        "        if not plant_dir.is_dir():\n",
        "            continue\n",
        "        plant = plant_dir.name\n",
        "        for disease_dir in plant_dir.iterdir():\n",
        "            if not disease_dir.is_dir():\n",
        "                continue\n",
        "            disease = disease_dir.name\n",
        "            for p in disease_dir.rglob(\"*\"):\n",
        "                if p.is_file() and p.suffix.lower() in EXTS:\n",
        "                    add_image(plant, disease, p)\n",
        "\n",
        "total_collected = sum(len(v) for v in groups.values())\n",
        "print(f\"Collected {total_collected} images across {len(groups)} (plant,disease) groups.\")\n",
        "if total_collected == 0:\n",
        "    raise RuntimeError(f\"No images found under {ROOT}.\")\n",
        "\n",
        "# 2) Build NEW splits into STAGING (copy files). We do not touch original splits yet.\n",
        "safe_rmtree(STAGING)\n",
        "(STAGING / \"train\").mkdir(parents=True, exist_ok=True)\n",
        "(STAGING / \"validation\").mkdir(parents=True, exist_ok=True)\n",
        "(STAGING / \"test\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def split_counts(n, f_train, f_val, f_test):\n",
        "    \"\"\"Return counts for train/val/test, with rounding safety.\"\"\"\n",
        "    n_train = int(np.floor(n * f_train))\n",
        "    n_val   = int(np.floor(n * f_val))\n",
        "    n_test  = n - n_train - n_val\n",
        "    if n_test < 0:\n",
        "        n_test = 0\n",
        "        n_val = n - n_train\n",
        "    return n_train, n_val, n_test\n",
        "\n",
        "copied = {\"train\":0, \"validation\":0, \"test\":0}\n",
        "\n",
        "for (plant, disease), files in sorted(groups.items()):\n",
        "    files = list(files)\n",
        "    rng.shuffle(files)\n",
        "    n_train, n_val, n_test = split_counts(len(files), TRAIN_FRAC, VAL_FRAC, TEST_FRAC)\n",
        "\n",
        "    subsets = {\n",
        "        \"train\": files[:n_train],\n",
        "        \"validation\": files[n_train:n_train+n_val],\n",
        "        \"test\": files[n_train+n_val:]\n",
        "    }\n",
        "\n",
        "    for split_name, subset in subsets.items():\n",
        "        dest_dir = STAGING / split_name / plant / disease\n",
        "        dest_dir.mkdir(parents=True, exist_ok=True)\n",
        "        for src in subset:\n",
        "            # Copy to staging (do not modify originals yet)\n",
        "            dst = dest_dir / Path(src).name\n",
        "            i = 1\n",
        "            while dst.exists():\n",
        "                dst = dest_dir / f\"{Path(src).stem}_{i}{Path(src).suffix}\"\n",
        "                i += 1\n",
        "            shutil.copy2(str(src), str(dst))\n",
        "            copied[split_name] += 1\n",
        "\n",
        "print(f\"Staging copies -> train: {copied['train']}, val: {copied['validation']}, test: {copied['test']}\")\n",
        "\n",
        "# 3) If staging succeeded, remove OLD final splits and move staging splits into place\n",
        "for final_split in [\"train\", \"validation\", \"test\"]:\n",
        "    safe_rmtree(ROOT / final_split)  # remove old target split completely\n",
        "    # move freshly built split from staging into ROOT\n",
        "    shutil.move(str(STAGING / final_split), str(ROOT / final_split))\n",
        "\n",
        "# 4) Cleanup staging root (now empty or only with non-moved dirs)\n",
        "safe_rmtree(STAGING)\n",
        "\n",
        "# 5) (Optional) Remove legacy source split folders that are not final names\n",
        "#     If you want to keep backups, comment this block out.\n",
        "for legacy in [\"val\", \"dev\", \"testing\"]:\n",
        "    safe_rmtree(ROOT / legacy)\n",
        "\n",
        "# 6) Print a simple summary and verify new structure counts\n",
        "def count_images(split_root: Path):\n",
        "    \"\"\"Count images under split_root recursively.\"\"\"\n",
        "    total = 0\n",
        "    for p in split_root.rglob(\"*\"):\n",
        "        if p.is_file() and p.suffix.lower() in EXTS:\n",
        "            total += 1\n",
        "    return total\n",
        "\n",
        "print(\"\\nRebuild COMPLETE.\")\n",
        "print(\"Final counts:\",\n",
        "      \"train:\", count_images(ROOT / \"train\"),\n",
        "      \"| validation:\", count_images(ROOT / \"validation\"),\n",
        "      \"| test:\", count_images(ROOT / \"test\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1NMuqW67dqv"
      },
      "source": [
        "Heare we creat tf.data data set with standart normalithation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZB1moakzp083"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "\n",
        "ROOT = Path(data_path) / \"image data\" if (Path(data_path) / \"image data\").exists() else Path(data_path)\n",
        "\n",
        "# Collect (path,label) pairs; label = plant name\n",
        "def collect_split(split):\n",
        "    paths, plants, diseases = [], [], []\n",
        "    split_dir = ROOT / split\n",
        "    if not split_dir.exists():\n",
        "        return paths, plants, diseases\n",
        "    for plant_dir in split_dir.iterdir():\n",
        "        if not plant_dir.is_dir():\n",
        "            continue\n",
        "        plant = plant_dir.name\n",
        "        for disease_dir in plant_dir.iterdir():\n",
        "            if not disease_dir.is_dir():\n",
        "                continue\n",
        "            disease = disease_dir.name\n",
        "            for p in disease_dir.rglob(\"*\"):\n",
        "                if p.is_file() and p.suffix.lower() in {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\",\".tif\",\".tiff\"}:\n",
        "                    paths.append(str(p))\n",
        "                    plants.append(plant)\n",
        "                    diseases.append(disease)\n",
        "    return paths, plants, diseases\n",
        "\n",
        "train_paths, train_plants, train_diseases = collect_split(\"train\")\n",
        "val_paths,   val_plants, val_diseases  = collect_split(\"validation\")\n",
        "test_paths,  test_plants, test_diseases  = collect_split(\"test\")\n",
        "\n",
        "# Build label mapping plantsload_image\n",
        "class_names_plants = sorted(set(train_plants + val_plants + test_plants))\n",
        "plants_to_index = {name:i for i,name in enumerate(class_names_plants)}\n",
        "train_plants = [plants_to_index[x] for x in train_plants]\n",
        "val_plants  = [plants_to_index[x] for x in val_plants]\n",
        "test_plants  = [plants_to_index[x] for x in test_plants]\n",
        "\n",
        "print(f\"Classes plants: {len(class_names_plants)} | Train {len(train_paths)} | Val {len(val_paths)} | Test {len(test_paths)}\")\n",
        "\n",
        "# Build label mapping plants\n",
        "class_names_disease = sorted(set(train_diseases + val_diseases + test_diseases))\n",
        "deseases_to_index = {name:i for i,name in enumerate(class_names_disease)}\n",
        "train_diseases = [deseases_to_index[x] for x in train_diseases]\n",
        "val_diseases   = [deseases_to_index[x] for x in val_diseases]\n",
        "test_diseases  = [deseases_to_index[x] for x in test_diseases]\n",
        "\n",
        "print(f\"Classes deseases: {len(class_names_disease)} | Train {len(train_paths)} | Val {len(val_paths)} | Test {len(test_paths)}\")\n",
        "\n",
        "# Parameters\n",
        "IMG_HEIGHT, IMG_WIDTH, BATCH_SIZE = 256, 256, 32\n",
        "\n",
        "def imagenet_normalize(image):\n",
        "    # Apply per-image standardization (zero mean, unit variance per image)\n",
        "    return tf.image.per_image_standardization(image * 255.0)\n",
        "\n",
        "def load_image(image_path, plants_label, diseases_label):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
        "    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    img = imagenet_normalize(img)\n",
        "    return img, (plants_label, diseases_label)\n",
        "\n",
        "# Build datasets\n",
        " # 0 if plants_label 1 if diseases_label 2 multi\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_plants, train_diseases)).map(load_image, num_parallel_calls=tf.data.AUTOTUNE) #.map(lambda img, labels: (img, labels[1]))\n",
        "train_ds = train_ds.shuffle(2048).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_paths, val_plants, val_diseases)).map(load_image, num_parallel_calls=tf.data.AUTOTUNE) #.map(lambda img, labels: (img, labels[1]))\n",
        "val_ds = val_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test_paths, test_plants, test_diseases)).map(load_image, num_parallel_calls=tf.data.AUTOTUNE) #.map(lambda img, labels: (img, labels[1]))\n",
        "test_ds = test_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "\n",
        "# Quick sanity check\n",
        "for name, ds in [(\"Train\", train_ds), (\"Validation\", val_ds), (\"Test\", test_ds)]:\n",
        "    try:\n",
        "        imgs, lbl = next(iter(ds))\n",
        "        # imgs, (lblp, lbld) = next(iter(ds)) #if we use  to labeles togater\n",
        "        #print(f\"\\n{name} batch: images {imgs.shape} | labels {lbl.shape} | dtype {imgs.dtype}\")\n",
        "        mn = tf.reduce_min(imgs).numpy().item()\n",
        "        mx = tf.reduce_max(imgs).numpy().item()\n",
        "        print(f\"{name} value range after per_image_standardization: [{mn:.3f}, {mx:.3f}]\")\n",
        "    except StopIteration:\n",
        "        print(f\"\\n{name}: EMPTY\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuQQ3AvOlAmL"
      },
      "outputs": [],
      "source": [
        "print(train_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCNv0_Q0zwe-"
      },
      "source": [
        "\n",
        "\n",
        "> Dublicate detection after spleating\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfcKmr19zodK"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from PIL import Image, ImageOps\n",
        "import imagehash\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "root = Path(data_path) / \"image data\"\n",
        "exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
        "\n",
        "def get_all_images():\n",
        "    #Get all images with metadata\n",
        "    images = []\n",
        "    for split in [\"train\", \"validation\", \"test\"]:\n",
        "        split_dir = root / split\n",
        "        if split_dir.exists():\n",
        "            for img_path in split_dir.rglob(\"*\"):\n",
        "                if img_path.suffix.lower() in exts:\n",
        "                    parts = img_path.relative_to(split_dir).parts\n",
        "                    images.append({\n",
        "                        'path': img_path,\n",
        "                        'split': split,\n",
        "                        'plant': parts[0] if parts else 'unknown',\n",
        "                        'name': img_path.name\n",
        "                    })\n",
        "    return images\n",
        "\n",
        "def compute_hash(img_path):\n",
        "    #Compute multiple hashes for better accuracy\n",
        "    try:\n",
        "        with Image.open(img_path) as img:\n",
        "            img = ImageOps.exif_transpose(img).convert(\"RGB\")  # Ensure 3 channels\n",
        "            # Use 3 different hash methods\n",
        "            phash = str(imagehash.phash(img, hash_size=12))\n",
        "            ahash = str(imagehash.average_hash(img, hash_size=12))\n",
        "            dhash = str(imagehash.dhash(img, hash_size=12))\n",
        "            return f\"{phash}_{ahash}_{dhash}\"\n",
        "    except Exception as e:\n",
        "        # print(f\"Error processing {img_path}: {e}\") # Optional: uncomment for debugging\n",
        "        return None\n",
        "\n",
        "def find_duplicates():\n",
        "    #Find all duplicates using hash comparison\n",
        "    print(\" Finding duplicates in all images...\")\n",
        "    # Get all images\n",
        "    all_images = get_all_images()\n",
        "    print(f\" Processing {len(all_images):,} images\")\n",
        "\n",
        "    # Compute hashes\n",
        "    hash_to_images = defaultdict(list)\n",
        "    for img_info in tqdm(all_images, desc=\"Computing hashes\"):\n",
        "        img_hash = compute_hash(img_info['path'])\n",
        "        if img_hash:\n",
        "            hash_to_images[img_hash].append(img_info)\n",
        "\n",
        "    # Find duplicate groups\n",
        "    duplicates = [group for group in hash_to_images.values() if len(group) > 1]\n",
        "    print(f\"\\n Results:\")\n",
        "    print(f\" Found {len(duplicates)} duplicate groups\")\n",
        "    print(f\" Total duplicates: {sum(len(group)-1 for group in duplicates)}\")\n",
        "    return duplicates\n",
        "\n",
        "def analyze_duplicates(duplicate_groups):\n",
        "    #Analyze and show duplicate patterns\n",
        "    cross_split = 0\n",
        "    same_split = 0\n",
        "    print(f\"\\n Duplicate Analysis:\")\n",
        "    for i, group in enumerate(duplicate_groups[:5]): # Show first 5 groups\n",
        "        splits = set(img['split'] for img in group)\n",
        "        if len(splits) > 1:\n",
        "            cross_split += 1\n",
        "            print(f\"\\n Group {i+1} (CROSS-SPLIT):\")\n",
        "        else:\n",
        "            same_split += 1\n",
        "            print(f\"\\n Group {i+1} (Same split: {list(splits)[0]}):\")\n",
        "        for img in group:\n",
        "            print(f\"  {img['split']}/{img['plant']}/{img['name']}\")\n",
        "    print(f\"\\n Summary:\")\n",
        "    print(f\"  Cross-split duplicates: {cross_split}\")\n",
        "    print(f\"  Same-split duplicates: {same_split}\")\n",
        "    return cross_split > 0\n",
        "\n",
        "def show_examples(duplicate_groups, num_examples=2):\n",
        "   #Show visual examples of duplicates\n",
        "    print(f\"\\n Visual Examples:\")\n",
        "    for i, group in enumerate(duplicate_groups[:num_examples]):\n",
        "        fig, axes = plt.subplots(1, min(len(group), 4), figsize=(12, 3))\n",
        "        if len(group) == 1:\n",
        "             axes = [axes] # Ensure axes is iterable even for single image group\n",
        "        for j, img in enumerate(group[:4]):\n",
        "            try:\n",
        "                with Image.open(img['path']) as pil_img:\n",
        "                    axes[j].imshow(pil_img)\n",
        "                    axes[j].set_title(f\"{img['split']}\\n{img['plant']}\", fontsize=9)\n",
        "                    axes[j].axis('off')\n",
        "            except:\n",
        "                axes[j].text(0.5, 0.5, \"Error\", ha='center')\n",
        "        plt.suptitle(f'Duplicate Group {i+1}')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def remove_duplicates(duplicate_groups):\n",
        "    #Remove duplicates keeping train > validation > test priority\n",
        "    priority = ['train', 'validation', 'test']\n",
        "    removed = 0\n",
        "    print(f\"\\n Removing duplicates (priority: {' > '.join(priority)}):\")\n",
        "    for group in duplicate_groups:\n",
        "        # Sort by priority\n",
        "        sorted_group = sorted(group, key=lambda x: priority.index(x['split']) if x['split'] in priority else 999)\n",
        "        # Keep first, remove others\n",
        "        keep = sorted_group[0]\n",
        "        remove_list = sorted_group[1:]\n",
        "\n",
        "        # Check if the image to keep actually exists before printing keep message\n",
        "        if keep['path'].exists():\n",
        "            print(f\" Keep: {keep['split']}/{keep['plant']}/{keep['name']}\")\n",
        "        else:\n",
        "             # If the image to keep doesn't exist, keep the next available one\n",
        "             print(f\" Warning: Image to keep {keep['path']} not found. Keeping the next available image in the group.\")\n",
        "             keep = None\n",
        "             for img in sorted_group:\n",
        "                 if img['path'].exists():\n",
        "                     keep = img\n",
        "                     print(f\" Keep: {keep['split']}/{keep['plant']}/{keep['name']}\")\n",
        "                     break\n",
        "             if keep is None:\n",
        "                 print(\" Error: No image to keep found in the group.\")\n",
        "                 continue # Skip to next group if no image to keep is found\n",
        "\n",
        "             remove_list = [img for img in sorted_group if img != keep]\n",
        "\n",
        "\n",
        "        for img in remove_list:\n",
        "            try:\n",
        "                if img['path'].exists(): # Check if file exists before attempting to remove\n",
        "                    img['path'].unlink()\n",
        "                    print(f\" Remove: {img['split']}/{img['plant']}/{img['name']}\")\n",
        "                    removed += 1\n",
        "                # else:\n",
        "                    # print(f\" Info: Image to remove {img['path']} not found (already removed?)\") # Optional: uncomment for debugging\n",
        "            except Exception as e:\n",
        "                print(f\"Error removing {img['path']}: {e}\") # Print specific error\n",
        "\n",
        "    print(f\"\\n Removed {removed} duplicate files\")\n",
        "    return removed\n",
        "\n",
        "# Execute duplicate detection\n",
        "print(\" SIMPLE DUPLICATE DETECTION\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Find duplicates\n",
        "duplicates = find_duplicates()\n",
        "\n",
        "if duplicates:\n",
        "    # Analyze patterns\n",
        "    has_cross_split = analyze_duplicates(duplicates)\n",
        "\n",
        "    # Show examples\n",
        "    show_examples(duplicates)\n",
        "\n",
        "    # Warning for cross-split duplicates\n",
        "    if has_cross_split:\n",
        "        print(f\"\\n WARNING: Cross-split duplicates found!\")\n",
        "        print(f\" This causes data leakage - model sees test data during training\")\n",
        "\n",
        "    # Ask before removing\n",
        "    print(f\"\\n Remove {sum(len(group)-1 for group in duplicates)} duplicate files?\")\n",
        "    print(f\" Uncomment the line below to proceed:\")\n",
        "    # print(f\" removed_count = remove_duplicates(duplicates)\")\n",
        "\n",
        "    # Uncomment to actually remove:\n",
        "    removed_count = remove_duplicates(duplicates)\n",
        "\n",
        "else:\n",
        "    print(\"No duplicates found!\")\n",
        "\n",
        "print(f\"\\n Detection completed!\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSOEKmPEatl4"
      },
      "source": [
        "#Weights and lost funcktions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKMFHBqJ2DSb"
      },
      "source": [
        "##Class weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZutRvW8N2Pfx"
      },
      "source": [
        "Becauuse we have strong inbalanse in ower data we diside to give proprotional to number of photo in ich class weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87pYsj3l2O8T"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "weights_plants = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=np.unique(train_plants),\n",
        "    y=train_plants\n",
        ")\n",
        "class_weights_plants = dict(enumerate(weights_plants))\n",
        "\n",
        "for idx, name in enumerate(class_weights_plants):\n",
        "    print(f\"Class {idx:2d} ({name:2d}) -> weight {class_weights_plants[idx]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKKqmWfYQoj-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# where train_labels_disease is an np.array of disease labels for the train\n",
        "# class_names_disease is a list of disease names (in the correct order)\n",
        "\n",
        "weights_disease = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=np.unique(train_diseases),\n",
        "    y=train_diseases\n",
        ")\n",
        "class_weights_disease = dict(enumerate(weights_disease))\n",
        "\n",
        "for idx, name in enumerate(class_names_disease):\n",
        "    print(f\"Disease {idx:2d} ({name:35s}) -> weight {class_weights_disease[idx]:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYcOAdcxaQe5"
      },
      "source": [
        "##Houdini Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DBQ8mAjaQSc"
      },
      "source": [
        "\n",
        "\n",
        "> Houdini loss function — this function simultaneously uses stochastic probabilities and standard distance metrics or gradients (Houdini = P(score_difference > threshold) × task_loss). It helps to evaluate better both distance factors and predictions, and gives better weights also in difficult cases and even in black boxes. This function was created in Israel.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcws9w7jaPpm"
      },
      "outputs": [],
      "source": [
        "# ===== H O U D I N I  L O S S  (standalone) =====\n",
        "# Comments are in English, talk to me in Russian.\n",
        "\n",
        "import math\n",
        "import torch\n",
        "\n",
        "def _normal_cdf(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Standard normal CDF using erf (stable and differentiable).\"\"\"\n",
        "    return 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "@torch.no_grad()\n",
        "def _ensure_long(t: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Ensure target dtype is long and detached (no grad).\"\"\"\n",
        "    return t.long()\n",
        "\n",
        "def houdini_loss(logits: torch.Tensor,\n",
        "                 target: torch.Tensor,\n",
        "                 sigma: float = 1.0,\n",
        "                 class_weight: torch.Tensor | None = None) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Weighted Houdini margin loss (per-sample weighting).\n",
        "    If class_weight is provided (Tensor[C]), scales each sample by class_weight[y_i].\n",
        "\n",
        "    Args:\n",
        "        logits: [B, C] raw scores\n",
        "        target: [B] class indices\n",
        "        sigma:  margin temperature\n",
        "        class_weight: [C] class weights aligned to 'classes' order\n",
        "\n",
        "    Returns:\n",
        "        Scalar loss (mean over batch)\n",
        "    \"\"\"\n",
        "    # AMP-safe: compute in float32 regardless of autocast\n",
        "    with torch.cuda.amp.autocast(enabled=False):\n",
        "        z = logits.float()\n",
        "        y = _ensure_long(target)\n",
        "\n",
        "        # true-class score s_y\n",
        "        s_y = z.gather(1, y.view(-1,1)).squeeze(1)  # [B]\n",
        "\n",
        "        # best non-true score s_other\n",
        "        mask = torch.ones_like(z, dtype=torch.bool)\n",
        "        mask.scatter_(1, y.view(-1,1), False)\n",
        "        s_other = z.masked_fill(~mask, float('-inf')).amax(dim=1)  # [B]\n",
        "\n",
        "        # margin (want negative)\n",
        "        m = (s_other - s_y) / float(sigma)  # [B]\n",
        "\n",
        "        # Φ(m) ∈ (0,1); higher when margin is positive (worse)\n",
        "        base = _normal_cdf(m)  # [B]\n",
        "\n",
        "        # optional per-sample scaling by class weights\n",
        "        if class_weight is not None:\n",
        "            w = class_weight.to(z.device, dtype=base.dtype).gather(0, y)  # [B]\n",
        "            base = base * w\n",
        "\n",
        "        return base.mean()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULiprOKAT9i4"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnCWWQL_JuQe"
      },
      "source": [
        "##CNN(convolution neyro networkong)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gin30paejN5_"
      },
      "source": [
        "Two experiments on 50% subset, 7 epochs,\n",
        "  A) Houdini-style margin loss\n",
        "  B) CrossEntropyLoss + class_weights\n",
        "CrossEntropyLoss mesure distanse bitween two probability distribution between real probabbility destribution: p and predicted probobility distrebiton of model: q, H(p,q) = -Σ p(x) log q(x).\n",
        "\n",
        "In most cases we dont need masure spcifik wites for Houdini because the Houdini weiths depends on the difference between the correct class and the \"strongest\" class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LZv35-Urg2U6"
      },
      "outputs": [],
      "source": [
        "# =================== UNIVERSAL BENCHMARK (plant vs disease) ===================\n",
        "# Compares Houdini vs CrossEntropy(+class weights) on chosen label granularity.\n",
        "# - Pluggable metrics: pass a list of callables (y_true, y_pred, y_proba, classes)->dict\n",
        "# - Supports user-provided class_weights dicts {class_name: weight}\n",
        "# - Keeps the best epoch by val macro-F1; returns predictions/probas for later analysis.\n",
        "\n",
        "import os, time, copy, math, warnings\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "\n",
        "# ---------------- User inputs ----------------\n",
        "ROOT = Path(data_path) / \"image data\" if (Path(data_path) / \"image data\").exists() else Path(data_path)\n",
        "\n",
        "SUBSAMPLE_FRAC = 0.10     # exact 20% of train\n",
        "EPOCHS         = 4       # keep tag text consistent below\n",
        "BATCH_SIZE     = 64\n",
        "NUM_WORKERS    = 2\n",
        "LR             = 3e-4\n",
        "WEIGHT_DECAY   = 1e-4\n",
        "RANDOM_SEED    = 42\n",
        "USE_REAL_HOUDINI = True   # if you already defined global houdini_loss(logits,y)\n",
        "\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---------------- Per-image standardization ----------------\n",
        "def per_image_standardization_torch(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Standardize a single image tensor channel-wise after scaling to [0,255].\"\"\"\n",
        "    x255 = x * 255.0\n",
        "    mean = x255.mean()\n",
        "    var  = x255.var(unbiased=False)\n",
        "    std  = torch.sqrt(var + 1e-12)\n",
        "    adjusted_std = torch.maximum(std, torch.tensor(1.0 / math.sqrt(x255.numel()), device=x.device, dtype=x.dtype))\n",
        "    return (x255 - mean) / adjusted_std\n",
        "\n",
        "class PathsDataset(Dataset):\n",
        "    \"\"\"Lightweight dataset built from arrays of paths and numeric labels.\"\"\"\n",
        "    def __init__(self, paths_np, labels_np, resize_hw=(256,256), augment=False):\n",
        "        self.paths_np = paths_np\n",
        "        self.labels_np = labels_np\n",
        "        self.resize = transforms.Resize(resize_hw, interpolation=transforms.InterpolationMode.BILINEAR)\n",
        "        self.augs = None\n",
        "        if augment:\n",
        "            self.augs = transforms.RandomOrder([\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomVerticalFlip(),\n",
        "                transforms.RandomRotation(15),\n",
        "                transforms.ColorJitter(0.2,0.2,0.2,0.03),\n",
        "            ])\n",
        "    def __len__(self): return len(self.paths_np)\n",
        "    def __getitem__(self,i):\n",
        "        p = self.paths_np[i]; y = int(self.labels_np[i])\n",
        "        img = Image.open(p).convert(\"RGB\")\n",
        "        img = self.resize(img)\n",
        "        x = transforms.functional.to_tensor(img)\n",
        "        if self.augs is not None:\n",
        "            x_pil = transforms.functional.to_pil_image(x)\n",
        "            x_pil = self.augs(x_pil)\n",
        "            x = transforms.functional.to_tensor(x_pil)\n",
        "        x = per_image_standardization_torch(x)\n",
        "        return x, y\n",
        "\n",
        "# ---------------- Split collection with label_mode ----------------\n",
        "EXTS = {'.jpg','.jpeg','.png','.bmp','.webp','.tif','.tiff'}\n",
        "\n",
        "def collect_split(split, label_mode=\"plant\"):\n",
        "    \"\"\"\n",
        "    label_mode='plant'   -> label = plant_dir.name\n",
        "    label_mode='disease' -> label = disease_dir.name  (merge same-named diseases)\n",
        "    \"\"\"\n",
        "    assert label_mode in {\"plant\",\"disease\"}\n",
        "    paths, labels = [], []\n",
        "    split_dir = ROOT / split\n",
        "    if not split_dir.exists():\n",
        "        return paths, labels\n",
        "    for plant_dir in sorted(p for p in split_dir.iterdir() if p.is_dir()):\n",
        "        plant = plant_dir.name\n",
        "        for disease_dir in sorted(p for p in plant_dir.iterdir() if p.is_dir()):\n",
        "            disease = disease_dir.name\n",
        "            for f in disease_dir.rglob(\"*\"):\n",
        "                if f.is_file() and f.suffix.lower() in EXTS:\n",
        "                    paths.append(str(f))\n",
        "                    labels.append(plant if label_mode==\"plant\" else disease)\n",
        "    return paths, labels\n",
        "\n",
        "def make_loaders(label_mode=\"plant\", augment_train=False):\n",
        "    \"\"\"Build datasets/dataloaders + numeric encodings for a chosen label granularity.\"\"\"\n",
        "    tr_p, tr_l_str = collect_split(\"train\", label_mode)\n",
        "    va_p, va_l_str = collect_split(\"validation\", label_mode)\n",
        "    te_p, te_l_str = collect_split(\"test\", label_mode)\n",
        "\n",
        "    classes = sorted(set(tr_l_str + va_l_str + te_l_str))\n",
        "    label_to_index = {c:i for i,c in enumerate(classes)}\n",
        "    tr_l = np.array([label_to_index[s] for s in tr_l_str], dtype=np.int64)\n",
        "    va_l = np.array([label_to_index[s] for s in va_l_str], dtype=np.int64)\n",
        "    te_l = np.array([label_to_index[s] for s in te_l_str], dtype=np.int64)\n",
        "\n",
        "    tr_p = np.array(tr_p); va_p = np.array(va_p); te_p = np.array(te_p)\n",
        "\n",
        "    train_ds_full = PathsDataset(tr_p, tr_l, augment=augment_train)\n",
        "    val_ds        = PathsDataset(va_p, va_l)\n",
        "    test_ds       = PathsDataset(te_p, te_l) if len(te_p) else None\n",
        "\n",
        "    # exact 20% subset\n",
        "    n_train = len(train_ds_full)\n",
        "    k = max(1, int(n_train * SUBSAMPLE_FRAC))\n",
        "    idx = np.arange(n_train)\n",
        "    rng = np.random.default_rng(RANDOM_SEED); rng.shuffle(idx)\n",
        "    train_indices = idx[:k]\n",
        "    train_ds = Subset(train_ds_full, train_indices)\n",
        "\n",
        "    pin_mem = (device.type=='cuda')\n",
        "    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=pin_mem)\n",
        "    val_dl   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=pin_mem)\n",
        "    test_dl  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=pin_mem) if test_ds else None\n",
        "\n",
        "    return dict(\n",
        "        classes=classes, C=len(classes),\n",
        "        train_ds_full=train_ds_full, train_ds=train_ds, train_indices=train_indices,\n",
        "        val_ds=val_ds, test_ds=test_ds,\n",
        "        train_dl=train_dl, val_dl=val_dl, test_dl=test_dl\n",
        "    )\n",
        "\n",
        "# ---------------- Model / metrics / loss helpers ----------------\n",
        "def make_resnet18(num_classes:int)->nn.Module:\n",
        "    \"\"\"ResNet-18 backbone with a light dropout on the head.\"\"\"\n",
        "    m = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "    m.fc = nn.Sequential(nn.Dropout(0.2), nn.Linear(m.fc.in_features, num_classes))\n",
        "    return m.to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_split(model, dl, criterion=None, need_probs:bool=True):\n",
        "    \"\"\"Evaluate on a dataloader; optionally collect probabilities for extra metrics.\"\"\"\n",
        "    model.eval()\n",
        "    tot=corr=0; loss_sum=0.0\n",
        "    y_true=[]; y_pred=[]; y_prob=[]\n",
        "    for x,y in dl:\n",
        "        x,y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        if criterion is not None:\n",
        "            loss_sum += criterion(logits,y).item()*y.size(0)\n",
        "        p = logits.argmax(1)\n",
        "        corr += (p==y).sum().item(); tot += y.size(0)\n",
        "        y_true.append(y.cpu().numpy()); y_pred.append(p.cpu().numpy())\n",
        "        if need_probs:\n",
        "            y_prob.append(torch.softmax(logits, dim=1).cpu().numpy())\n",
        "    if tot==0:\n",
        "        return dict(\n",
        "            loss=0.0, acc=0.0, f1_macro=0.0, f1_weighted=0.0,\n",
        "            y_true=np.array([]), y_pred=np.array([]), y_proba=np.array([])\n",
        "        )\n",
        "    y_true = np.concatenate(y_true); y_pred = np.concatenate(y_pred)\n",
        "    y_proba = np.concatenate(y_prob) if (need_probs and len(y_prob)) else None\n",
        "    acc = corr/tot\n",
        "    f1m = f1_score(y_true,y_pred,average='macro',zero_division=0)\n",
        "    f1w = f1_score(y_true,y_pred,average='weighted',zero_division=0)\n",
        "    loss_avg = loss_sum/tot if criterion is not None else 0.0\n",
        "    return dict(loss=loss_avg, acc=acc, f1_macro=f1m, f1_weighted=f1w,\n",
        "                y_true=y_true, y_pred=y_pred, y_proba=y_proba)\n",
        "\n",
        "def align_weights_from_dict(classes, weights_dict:dict|None):\n",
        "    \"\"\"\n",
        "    Align a user-provided dict {class_name: weight} to a torch tensor in `classes` order.\n",
        "    Warn if there are missing or extra keys. If missing, fallback to 1.0.\n",
        "    \"\"\"\n",
        "    if not weights_dict:\n",
        "        return None\n",
        "    miss = [c for c in classes if c not in weights_dict]\n",
        "    extra = [k for k in weights_dict.keys() if k not in classes]\n",
        "    if miss:\n",
        "        warnings.warn(f\"[class weights] missing classes: {miss}; using 1.0 for them\")\n",
        "    if extra:\n",
        "        warnings.warn(f\"[class weights] extra keys not in classes: {extra} (ignored)\")\n",
        "    arr = np.array([weights_dict.get(c, 1.0) for c in classes], dtype=np.float32)\n",
        "    return torch.tensor(arr, device=device)\n",
        "\"\"\"\n",
        "class HoudiniMarginLoss(nn.Module):\n",
        "    #Self-contained Houdini-style margin loss (if you don't use your own)\n",
        "    def __init__(self, tau: float = 1.0):\n",
        "        super().__init__(); self.tau=tau; self.sigmoid=nn.Sigmoid()\n",
        "    def forward(self, logits, y):\n",
        "        s_true = logits.gather(1, y.view(-1,1))\n",
        "        mask = torch.ones_like(logits, dtype=torch.bool); mask.scatter_(1, y.view(-1,1), False)\n",
        "        max_other = logits.masked_fill(~mask, float('-inf')).amax(dim=1, keepdim=True)\n",
        "        margin = (max_other - s_true)/self.tau\n",
        "        return self.sigmoid(margin).mean()\n",
        "\"\"\"\n",
        "def run_experiment_pair(label_mode=\"plant\",\n",
        "                        class_weights_dict:dict|None=None,\n",
        "                        metrics_fns:list|None=None,\n",
        "                        augment_train:bool=False):\n",
        "    \"\"\"\n",
        "    Run two training loops on the same split/subset:\n",
        "      - 'houdini' : your global houdini_loss if available (or local fallback)\n",
        "      - 'cew'     : CrossEntropy with aligned class weights (if provided)\n",
        "    metrics_fns: list of callables (y_true, y_pred, y_proba, classes) -> dict\n",
        "    Returns a dict with per-mode summaries, predictions, and models.\n",
        "    \"\"\"\n",
        "    # loaders\n",
        "    bundle = make_loaders(label_mode=label_mode, augment_train=augment_train)\n",
        "    classes = bundle[\"classes\"]; C=bundle[\"C\"]\n",
        "    val_dl=bundle[\"val_dl\"]; test_dl=bundle[\"test_dl\"]\n",
        "\n",
        "    # class weights for CE aligned to class order\n",
        "    CE_WEIGHTS = align_weights_from_dict(classes, class_weights_dict)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for mode in (\"houdini\",\"cew\"):\n",
        "        model = make_resnet18(C)\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "    mode='max',\n",
        "    factor=0.5,\n",
        "    patience=4,\n",
        "    min_lr=1e-6,\n",
        "    #verbose=True\n",
        ")\n",
        "        scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n",
        "        pct = int(SUBSAMPLE_FRAC * 100)\n",
        "        if mode==\"houdini\":\n",
        "            if USE_REAL_HOUDINI and ('houdini_loss' in globals()):\n",
        "                def criterion(logits,y): return houdini_loss(logits,y)  # your custom\n",
        "            else:\n",
        "                criterion = HoudiniMarginLoss(tau=1.0)\n",
        "            tag = f\"{label_mode.upper()} | Houdini_{pct}p_{EPOCHS}e\"\n",
        "        else:\n",
        "            criterion = nn.CrossEntropyLoss(weight=CE_WEIGHTS) if CE_WEIGHTS is not None else nn.CrossEntropyLoss()\n",
        "            tag = f\"{label_mode.upper()} | CE+Weights_{pct}p_{EPOCHS}e\" if CE_WEIGHTS is not None else f\"{label_mode.upper()} | CE_20p_{EPOCHS}e\"\n",
        "\n",
        "        best_f1m=-1.0; best_state=None; best_epoch=-1\n",
        "\n",
        "        def train_one_epoch():\n",
        "            \"\"\"One training epoch with AMP.\"\"\"\n",
        "            model.train()\n",
        "            tot=corr=0; loss_sum=0.0\n",
        "            for x,y in bundle[\"train_dl\"]:\n",
        "                x,y = x.to(device), y.to(device)\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
        "                    logits = model(x); loss = criterion(logits,y)\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer); scaler.update()\n",
        "                with torch.no_grad():\n",
        "                    p = logits.argmax(1); corr += (p==y).sum().item(); tot += y.size(0); loss_sum += loss.item()*y.size(0)\n",
        "            return (loss_sum/max(1,tot), corr/max(1,tot))\n",
        "\n",
        "        # --- train loop\n",
        "        for epoch in range(1,EPOCHS+1):\n",
        "            t0=time.time()\n",
        "            tr_loss, tr_acc = train_one_epoch()\n",
        "            val_stats = eval_split(model, val_dl, criterion, need_probs=True)\n",
        "            # после val_stats = eval_split(...)\n",
        "            scheduler.step(val_stats['f1_macro'])  # для mode='max'\n",
        "\n",
        "            print(f\"[{tag}] epoch {epoch:02d} | train acc {tr_acc:.3f} loss {tr_loss:.3f} | \"\n",
        "                  f\"val acc {val_stats['acc']:.3f} f1m {val_stats['f1_macro']:.3f} loss {val_stats['loss']:.3f} | time {time.time()-t0:.1f}s\")\n",
        "            if val_stats['f1_macro'] > best_f1m:\n",
        "                best_f1m = float(val_stats['f1_macro']); best_epoch=epoch; best_state=copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if best_state is not None:\n",
        "            model.load_state_dict(best_state)\n",
        "\n",
        "        # --- final evals\n",
        "        val_stats  = eval_split(model, val_dl,  criterion, need_probs=True)\n",
        "        test_stats = eval_split(model, test_dl, criterion, need_probs=True) if test_dl is not None else None\n",
        "\n",
        "        # --- built-in prints\n",
        "        print(\"\\nValidation classification report:\")\n",
        "        print(classification_report(val_stats['y_true'], val_stats['y_pred'], target_names=classes, zero_division=0))\n",
        "        if test_stats is not None:\n",
        "            print(\"Test classification report:\")\n",
        "            print(classification_report(test_stats['y_true'], test_stats['y_pred'], target_names=classes, zero_division=0))\n",
        "\n",
        "        # --- user metrics (plug-in)\n",
        "        extra_val_metrics = {}\n",
        "        extra_test_metrics = {}\n",
        "        if metrics_fns:\n",
        "            for fn in metrics_fns:\n",
        "                try:\n",
        "                    extra_val_metrics |= fn(val_stats['y_true'], val_stats['y_pred'], val_stats['y_proba'], classes)\n",
        "                    if test_stats is not None:\n",
        "                        extra_test_metrics |= fn(test_stats['y_true'], test_stats['y_pred'], test_stats['y_proba'], classes)\n",
        "                except Exception as e:\n",
        "                    warnings.warn(f\"[metrics] '{getattr(fn,'__name__',str(fn))}' failed: {e}\")\n",
        "\n",
        "        summary = dict(\n",
        "            label_mode=label_mode, tag=tag, best_epoch=best_epoch,\n",
        "            final_val_f1_macro=float(val_stats['f1_macro']),\n",
        "            final_val_loss=float(val_stats['loss']),\n",
        "            final_val_acc=float(val_stats['acc']),\n",
        "            final_test_f1_macro=float(test_stats['f1_macro']) if test_stats is not None else float('nan'),\n",
        "            final_test_acc=float(test_stats['acc']) if test_stats is not None else float('nan'),\n",
        "            extra_val_metrics=extra_val_metrics,\n",
        "            extra_test_metrics=extra_test_metrics\n",
        "        )\n",
        "\n",
        "        results[mode] = dict(\n",
        "            summary=summary,\n",
        "            model=model,\n",
        "            classes=classes,\n",
        "            val_stats=val_stats,\n",
        "            test_stats=test_stats\n",
        "        )\n",
        "\n",
        "    # comparison print\n",
        "    print(f\"\\n=== COMPARISON [{label_mode.upper()}] (20% / {EPOCHS} epochs) ===\")\n",
        "    h = results[\"houdini\"][\"summary\"]; c = results[\"cew\"][\"summary\"]\n",
        "    show = ['best_epoch','final_val_f1_macro','final_val_loss','final_val_acc','final_test_f1_macro','final_test_acc']\n",
        "    print(\"Houdini: \", {k:h[k] for k in show})\n",
        "    print(\"CE+Wght:\", {k:c[k] for k in show})\n",
        "    return results\n",
        "\n",
        "# ---------------- Example: wrap your custom metrics ----------------\n",
        "# Define adapters for your already-written metrics. Each should return a dict.\n",
        "def metric_confmat(y_true, y_pred, y_proba, classes):\n",
        "    \"\"\"Return confusion matrix as a flattened dict for logging (optional).\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=np.arange(len(classes)))\n",
        "    return {\"confusion_matrix\": cm.tolist()}  # keep it JSON-serializable\n",
        "\n",
        "# If you have your own functions, pass them here (example):\n",
        "# metrics_list = [your_metric_fn1, your_metric_fn2, metric_confmat]\n",
        "metrics_list = [metric_confmat]\n",
        "\n",
        "# ---------------- RUNS ----------------\n",
        "# IMPORTANT: pass the corresponding weights dict for each label_mode.\n",
        "#   - class_weights_plants:   {plant_name: weight}\n",
        "#   - class_weights_diseases: {disease_name: weight}\n",
        "#\n",
        "# Example (uncomment when you have both dicts in scope):\n",
        "res_plant   = run_experiment_pair(label_mode=\"plant\",\n",
        "                                   class_weights_dict=class_weights_plants,\n",
        "                                   metrics_fns=metrics_list,\n",
        "                                   augment_train=False)\n",
        "res_disease = run_experiment_pair(label_mode=\"disease\",\n",
        "                                   class_weights_dict=class_weights_disease,\n",
        "                                   metrics_fns=metrics_list,\n",
        "                                   augment_train=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-3kZ7RaneBO"
      },
      "source": [
        "Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEGWsC8oLPaV"
      },
      "outputs": [],
      "source": [
        "# ================== PLOTS ONLY (no training) ==================\n",
        "# This cell expects the variables `res_plant` and `res_disease` to exist in scope.\n",
        "# It DOES NOT train anything — it only visualizes existing results.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---- Utilities to safely extract histories (if present) ----\n",
        "def _has_history(res):\n",
        "    \"\"\"Return True if both modes contain 'history' with per-epoch dicts.\"\"\"\n",
        "    try:\n",
        "        for mode in (\"houdini\", \"cew\"):\n",
        "            h = res[mode].get(\"history\", None)\n",
        "            if not isinstance(h, list) or len(h) == 0:\n",
        "                return False\n",
        "            # Check that the first item looks like a per-epoch dict\n",
        "            if not isinstance(h[0], dict) or \"epoch\" not in h[0]:\n",
        "                return False\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def _series_from(res, label_prefix: str):\n",
        "    \"\"\"\n",
        "    Convert results[mode]['history'] into a dict of metric-> {label: (epochs, values)}.\n",
        "    Expected metric keys in history: 'train_acc','train_loss','val_acc','val_f1m','val_loss'\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    for mode, nice in ((\"houdini\", f\"{label_prefix}–Houdini\"),\n",
        "                       (\"cew\",     f\"{label_prefix}–CEW\")):\n",
        "        hist = res[mode][\"history\"]\n",
        "        epochs = [d.get(\"epoch\", i+1) for i,d in enumerate(hist)]\n",
        "        for key_src, key_dst in (\n",
        "            (\"train_acc\", \"train_acc\"),\n",
        "            (\"train_loss\",\"train_loss\"),\n",
        "            (\"val_acc\",   \"val_acc\"),\n",
        "            (\"val_f1m\",   \"val_f1m\"),\n",
        "            (\"val_loss\",  \"val_loss\"),\n",
        "        ):\n",
        "            vals = [d[key_src] for d in hist if key_src in d]\n",
        "            if len(vals) == len(epochs):\n",
        "                out.setdefault(key_dst, {})\n",
        "                out[key_dst][nice] = (epochs, vals)\n",
        "    return out\n",
        "\n",
        "def _plot_lines(metric_title: str, key: str, plant_series: dict, disease_series: dict):\n",
        "    \"\"\"Draw a line plot with 4 lines: Plant–Houdini, Plant–CEW, Disease–Houdini, Disease–CEW.\"\"\"\n",
        "    plt.figure()\n",
        "    plotted = 0\n",
        "    for label in (\"Plant–Houdini\",\"Plant–CEW\",\"Disease–Houdini\",\"Disease–CEW\"):\n",
        "        src = plant_series if label.startswith(\"Plant\") else disease_series\n",
        "        if key in src and label in src[key]:\n",
        "            xs, ys = src[key][label]\n",
        "            plt.plot(xs, ys, marker='o', label=label)\n",
        "            plotted += 1\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(metric_title.split()[-1] if \"Loss\" not in metric_title else \"Loss\")\n",
        "    plt.title(metric_title)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    if plotted:\n",
        "        plt.legend()\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, \"No per-epoch history found\", ha='center', va='center', transform=plt.gca().transAxes)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ---- Fallback: bar charts from summaries (no history available) ----\n",
        "def _bar_from_summaries(res_plant, res_disease, title: str, extract_key: str):\n",
        "    \"\"\"\n",
        "    Make a 4-bar comparison (Plant–Houdini, Plant–CEW, Disease–Houdini, Disease–CEW)\n",
        "    using values from results[mode]['summary'][extract_key].\n",
        "    \"\"\"\n",
        "    labels = [\"Plant–Houdini\",\"Plant–CEW\",\"Disease–Houdini\",\"Disease–CEW\"]\n",
        "    values = []\n",
        "    for res in (res_plant, res_plant, res_disease, res_disease):\n",
        "        mode = \"houdini\" if len(values) % 2 == 0 else \"cew\"\n",
        "        val = float(res[mode][\"summary\"].get(extract_key, np.nan))\n",
        "        values.append(val)\n",
        "\n",
        "    x = np.arange(len(labels))\n",
        "    plt.figure()\n",
        "    plt.bar(x, values)\n",
        "    plt.xticks(x, labels, rotation=20, ha='right')\n",
        "    plt.title(title)\n",
        "    plt.ylabel(extract_key.replace(\"_\", \" \").title())\n",
        "    for xi, v in zip(x, values):\n",
        "        plt.text(xi, v, f\"{v:.3f}\" if np.isfinite(v) else \"NA\", ha='center', va='bottom', fontsize=8)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ---- Decide which path to take (with or without history) ----\n",
        "plant_has_hist   = _has_history(res_plant)\n",
        "disease_has_hist = _has_history(res_disease)\n",
        "both_have_hist   = plant_has_hist and disease_has_hist\n",
        "\n",
        "if both_have_hist:\n",
        "    # ------------- Line plots: 4 charts × 4 lines -------------\n",
        "    plant_series   = _series_from(res_plant,   \"Plant\")\n",
        "    disease_series = _series_from(res_disease, \"Disease\")\n",
        "\n",
        "    # 1) Train Accuracy\n",
        "    _plot_lines(\"Train Accuracy\", \"train_acc\", plant_series, disease_series)\n",
        "    # 2) Train Loss\n",
        "    _plot_lines(\"Train Loss\", \"train_loss\", plant_series, disease_series)\n",
        "    # 3) Validation Accuracy\n",
        "    _plot_lines(\"Validation Accuracy\", \"val_acc\", plant_series, disease_series)\n",
        "    # 4) Validation F1 (macro)\n",
        "    _plot_lines(\"Validation F1 (macro)\", \"val_f1m\", plant_series, disease_series)\n",
        "\n",
        "else:\n",
        "    # ------------- Fallback (no history): bar comparisons by summaries -------------\n",
        "    # We can't reconstruct per-epoch curves. Show final comparisons instead.\n",
        "    print(\"[info] No per-epoch history found in results. Showing final-summary comparisons only.\")\n",
        "    # 1) Validation Accuracy\n",
        "    _bar_from_summaries(res_plant, res_disease, \"Validation Accuracy (final)\", \"final_val_acc\")\n",
        "    # 2) Validation F1 (macro)\n",
        "    _bar_from_summaries(res_plant, res_disease, \"Validation F1 (macro, final)\", \"final_val_f1_macro\")\n",
        "    # 3) Validation Loss\n",
        "    _bar_from_summaries(res_plant, res_disease, \"Validation Loss (final)\", \"final_val_loss\")\n",
        "    # 4) Test Accuracy (if NaN for some mode, it'll show NA)\n",
        "    _bar_from_summaries(res_plant, res_disease, \"Test Accuracy (final)\", \"final_test_acc\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dopXpnC5pY9"
      },
      "source": [
        "Plants recognation(its heppend because plants clasification in ower data its easy task)\n",
        "The model shows very high accuracy on both train and validation with both loss functions. That is a good sign that we don’t have overfitting (we also checked that we do not have data leakage). The loss is very low, which means the model detects patterns well.\n",
        "\n",
        "F1 is close to 1, which shows we have a very good balance between precision and recall. It means the model works well with both rare and common classes. That means in the case with CrossEntropyLoss + class_weights, our class_weights were enough to fix the imbalance that we can see in cases with Random Forest and Logistic Regression.\n",
        "\n",
        "Desease recognation\n",
        "All resultes lower and worster but still not bed wi both model. Hodini have better loss bud CE+W have beter F1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4-Z8SmXAgqi"
      },
      "source": [
        "## Random Forest and Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oT7T3pbHAlnE"
      },
      "source": [
        "We diside to use first off all \"classical\" model to compare with NW models like CNN for learning resons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF7HWBiCFnJL"
      },
      "source": [
        "\n",
        "\n",
        "> Embeding by Histogram of oriented gradients(by HOG we create tipe of map with gradient directions thet distrebiut by colors and britnes chenging give direction for each pixel itch pixel), also we use heare PCA for redusing dimantion. And HSV its clor scheme(Hue -color, Saturation, Value- britness)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBX6iIq6F8Gz"
      },
      "source": [
        "\n",
        "\n",
        "> Also we use F1 score metric: 2*(Precision*Recall)/(Precision+Recall)\n",
        "Precision and Recall we got from confusion matrix\n",
        "Precision is TP/(TP+FP). Its mean corecct positive prediction divide all positive predicton, its mean from all positive prediction how many is realy positive. Show accurascy of prediction.\n",
        "Recall is TP/(TP+FN). Its mean correct positive prediction divede all positive cases its mean from all positive cases, how many we successfully found. Show sensitivity of prediction.\n",
        "F1 overall is quality of positive class classification.\n",
        "F1 macro = is F1 score mean for all classes (same waite for all classes)\n",
        "F1-weighted = weighted averaging for all classes (weight of each class is proportional to the number of examples in that class)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9Tq3AnRSFmZW"
      },
      "outputs": [],
      "source": [
        "# === Slim HOG+PCA baseline (end-to-end, robust to (plant,disease) dual labels) ===\n",
        "# All comments are in English, per your request.\n",
        "\n",
        "import numpy as np, random, math, warnings\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import feature\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "\n",
        "# Optional frameworks (the code works even if they're missing)\n",
        "try:\n",
        "    import torch\n",
        "except Exception:\n",
        "    torch = None\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "except Exception:\n",
        "    tf = None\n",
        "\n",
        "# ----------------------------\n",
        "# Config\n",
        "# ----------------------------\n",
        "TRAIN_FRACTION     = 0.20         # fraction of train_ds to subsample for PCA/train\n",
        "RANDOM_SEED        = 42\n",
        "N_COMPONENTS       = 156\n",
        "USE_RANDOM_FOREST  = True\n",
        "\n",
        "HOG_PARAMS = dict(\n",
        "    orientations=9,\n",
        "    pixels_per_cell=(16, 16),\n",
        "    cells_per_block=(2, 2),\n",
        "    block_norm='L2-Hys',\n",
        "    feature_vector=True\n",
        ")\n",
        "\n",
        "rng = np.random.RandomState(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "if torch is not None:\n",
        "    try: torch.manual_seed(RANDOM_SEED)\n",
        "    except: pass\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "# Optional: define class_weights externally as a dict {class_id: weight} or None.\n",
        "# Example:\n",
        "# class_weights = {0: 1.0, 1: 2.0, 2: 1.3}\n",
        "# If not defined, it will be ignored.\n",
        "if 'class_weights' not in globals():\n",
        "    class_weights = None\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Minimal helpers (framework-agnostic)\n",
        "# ----------------------------\n",
        "def _to_numpy(x):\n",
        "    \"\"\"Convert torch/tf/PIL/array-like to numpy array.\"\"\"\n",
        "    if torch is not None and isinstance(x, getattr(torch, \"Tensor\", ())):\n",
        "        return x.detach().cpu().numpy()\n",
        "    if tf is not None and isinstance(x, getattr(tf, \"Tensor\", ())):\n",
        "        return x.numpy()\n",
        "    try:\n",
        "        from PIL import Image as _PILImage\n",
        "        if isinstance(x, _PILImage.Image):\n",
        "            return np.asarray(x)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return np.asarray(x)\n",
        "\n",
        "def _hwc_from_any(img):\n",
        "    \"\"\"Ensure image shape is HWC or HW (with channel axis last).\"\"\"\n",
        "    arr = _to_numpy(img)\n",
        "    if arr.ndim == 2:\n",
        "        return arr[..., None]\n",
        "    if arr.ndim == 3:\n",
        "        # Convert CHW to HWC if needed\n",
        "        if arr.shape[0] in (1,3) and arr.shape[0] < arr.shape[-1]:\n",
        "            return np.transpose(arr, (1,2,0))  # (C,H,W)->(H,W,C)\n",
        "        return arr\n",
        "    raise ValueError(f\"Unsupported image shape: {arr.shape}\")\n",
        "\n",
        "def batch_hog(images):\n",
        "    \"\"\"Compute HOG features for a batch of images (HWC or HW).\"\"\"\n",
        "    arr = _to_numpy(images)\n",
        "    if arr.ndim == 2:\n",
        "        arr = arr[None, ..., None]  # -> (1,H,W,1)\n",
        "    elif arr.ndim == 3:\n",
        "        arr = arr[None, ...]        # -> (1,H,W,C)\n",
        "    elif arr.ndim != 4:\n",
        "        raise ValueError(f\"Expected 2D/3D/4D, got {arr.shape}\")\n",
        "\n",
        "    feats = []\n",
        "    for i in range(arr.shape[0]):\n",
        "        img = arr[i]\n",
        "        if img.ndim == 3 and img.shape[-1] > 1:\n",
        "            gray = np.mean(img.astype(np.float64), axis=-1)\n",
        "        elif img.ndim == 3:\n",
        "            gray = img[..., 0].astype(np.float64)\n",
        "        else:\n",
        "            gray = img.astype(np.float64)\n",
        "        feats.append(feature.hog(gray, **HOG_PARAMS).astype(np.float32))\n",
        "    return np.asarray(feats, dtype=np.float32)\n",
        "\n",
        "def _is_indexable_dataset(ds):\n",
        "    \"\"\"Return True if ds supports __len__ and __getitem__.\"\"\"\n",
        "    return hasattr(ds, '__len__') and hasattr(ds, '__getitem__')\n",
        "\n",
        "def _unwrap_example(example):\n",
        "    \"\"\"Extract (image, label) from common dataset formats.\"\"\"\n",
        "    if isinstance(example, (tuple, list)) and len(example) >= 2:\n",
        "        return example[0], example[1]\n",
        "    if isinstance(example, dict):\n",
        "        for ki in ('image','img','x','features'):\n",
        "            for kl in ('label','y','target','labels','targets'):\n",
        "                if ki in example and kl in example:\n",
        "                    return example[ki], example[kl]\n",
        "    raise ValueError(\"Provide (image,label) or dict with image/label.\")\n",
        "\n",
        "def _split_dataset(ds, val_frac=0.15, test_frac=0.15, seed=RANDOM_SEED):\n",
        "    \"\"\"Split indexable dataset into (train,val,test) subsets.\"\"\"\n",
        "    if not _is_indexable_dataset(ds):\n",
        "        raise ValueError(\"Need an indexable dataset (has __len__ and __getitem__). \"\n",
        "                         \"If you have DataLoaders or tf.data, pass val/test explicitly.\")\n",
        "    n = len(ds)\n",
        "    idx = np.arange(n)\n",
        "    local_rng = np.random.RandomState(seed)\n",
        "    local_rng.shuffle(idx)\n",
        "    n_test = int(round(n * test_frac))\n",
        "    n_val  = int(round(n * val_frac))\n",
        "    test_idx = idx[:n_test]\n",
        "    val_idx  = idx[n_test:n_test+n_val]\n",
        "    train_idx = idx[n_test+n_val:]\n",
        "\n",
        "    # Try torch Subset; else use a lightweight fallback\n",
        "    try:\n",
        "        import torch.utils.data as _tud\n",
        "        return (_tud.Subset(ds, train_idx), _tud.Subset(ds, val_idx), _tud.Subset(ds, test_idx))\n",
        "    except Exception:\n",
        "        class _Subset:\n",
        "            def __init__(self, base, indices): self.base, self.indices = base, list(indices)\n",
        "            def __len__(self): return len(self.indices)\n",
        "            def __getitem__(self, i): return self.base[self.indices[i]]\n",
        "        base = ds\n",
        "        return (_Subset(base, train_idx), _Subset(base, val_idx), _Subset(base, test_idx))\n",
        "\n",
        "def _select_label(lab, label_mode):\n",
        "    \"\"\"\n",
        "    Return a single integer label from many possible shapes:\n",
        "      - scalar\n",
        "      - 1D vector with >=2 elements (index 0=plant, 1=disease)\n",
        "      - dict {'plant':..., 'disease':...}\n",
        "      - tuple/list of two scalars (plant, disease)\n",
        "    \"\"\"\n",
        "    # tuple/list of two scalars\n",
        "    if isinstance(lab, (tuple, list)) and len(lab) >= 2 and not hasattr(lab[0], \"__len__\"):\n",
        "        return int(lab[0] if label_mode == 'plant' else lab[1])\n",
        "\n",
        "    if isinstance(lab, dict):\n",
        "        key = 'plant' if label_mode == 'plant' else 'disease'\n",
        "        return int(_to_numpy(lab[key]))\n",
        "\n",
        "    arr = _to_numpy(lab)\n",
        "    if arr.ndim == 0:\n",
        "        return int(arr)\n",
        "    if arr.ndim == 1 and arr.size >= 2:\n",
        "        idx = 0 if label_mode == 'plant' else 1\n",
        "        return int(arr[idx])\n",
        "    return int(arr.reshape(()))\n",
        "\n",
        "\n",
        "def _labels_from_any(yb, label_mode):\n",
        "    \"\"\"\n",
        "    Convert batched labels of many shapes to a 1D numpy int64 vector of length B.\n",
        "    Supported inputs (batched):\n",
        "      - yb shape (B,) : already single task\n",
        "      - yb shape (B,2) : [:,0]=plant, [:,1]=disease\n",
        "      - tuple/list (plant_batch, disease_batch), each shape (B,)\n",
        "      - dict {'plant': batch, 'disease': batch}\n",
        "    \"\"\"\n",
        "    # tuple/list of two batched tensors\n",
        "    if isinstance(yb, (tuple, list)) and len(yb) >= 2 and hasattr(yb[0], \"__len__\"):\n",
        "        a = _to_numpy(yb[0])\n",
        "        b = _to_numpy(yb[1])\n",
        "        y = a if label_mode == 'plant' else b\n",
        "        return y.astype(np.int64).reshape(-1)\n",
        "\n",
        "    if isinstance(yb, dict):\n",
        "        key = 'plant' if label_mode == 'plant' else 'disease'\n",
        "        y = _to_numpy(yb[key])\n",
        "        return y.astype(np.int64).reshape(-1)\n",
        "\n",
        "    arr = _to_numpy(yb)\n",
        "    # (B,2) -> pick a column\n",
        "    if arr.ndim == 2 and arr.shape[1] >= 2:\n",
        "        idx = 0 if label_mode == 'plant' else 1\n",
        "        arr = arr[:, idx]\n",
        "    # (B,1) or other >1D → flatten to (B,)\n",
        "    if arr.ndim > 1:\n",
        "        arr = arr.reshape(-1)\n",
        "    return arr.astype(np.int64)\n",
        "\n",
        "\n",
        "def collect_subset_hog(ds, take_fraction, rng, label_mode=\"plant\"):\n",
        "    \"\"\"\n",
        "    Collect a subsample of HOG features and labels from 'ds'.\n",
        "    Works with indexable datasets and iterable/batched sources (DataLoader / tf.data).\n",
        "    \"\"\"\n",
        "    X_list, y_list = [], []\n",
        "\n",
        "    def _push(img, y_scalar):\n",
        "        feat = batch_hog(_hwc_from_any(img)[None, ...])\n",
        "        X_list.append(feat)\n",
        "        y_list.append(np.int64(y_scalar).reshape(-1))\n",
        "\n",
        "    if _is_indexable_dataset(ds):\n",
        "        n = len(ds)\n",
        "        k = max(1, int(n * take_fraction))\n",
        "        for i in rng.choice(n, size=k, replace=False):\n",
        "            img, lab = _unwrap_example(ds[i])\n",
        "            y_scalar = _select_label(lab, label_mode)\n",
        "            _push(img, y_scalar)\n",
        "    else:\n",
        "        target = 5000  # soft cap to avoid scanning entire infinite streams\n",
        "        collected = 0\n",
        "        for ex in ds:\n",
        "            try:\n",
        "                xb, yb = _unwrap_example(ex)\n",
        "                xb = _to_numpy(xb)\n",
        "                # Normalize images to (B,H,W,C?)\n",
        "                if xb.ndim == 4 and xb.shape[1] in (1, 3) and xb.shape[1] < xb.shape[-1]:\n",
        "                    xb = np.transpose(xb, (0, 2, 3, 1))  # CHW -> HWC\n",
        "                elif xb.ndim == 3:\n",
        "                    xb = xb[None, ...]\n",
        "                if xb.ndim != 4:\n",
        "                    continue\n",
        "\n",
        "                # Get a 1D label vector of length B\n",
        "                yb_vec = _labels_from_any(yb, label_mode)\n",
        "                B = xb.shape[0]\n",
        "                if yb_vec.shape[0] != B:\n",
        "                    # Skip malformed batch\n",
        "                    continue\n",
        "\n",
        "                for i in range(B):\n",
        "                    if rng.rand() <= take_fraction:\n",
        "                        _push(xb[i], yb_vec[i])\n",
        "                        collected += 1\n",
        "                        if collected >= target:\n",
        "                            raise StopIteration\n",
        "            except StopIteration:\n",
        "                break\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "    if not X_list:\n",
        "        dummy = feature.hog(np.zeros((32,32), dtype=np.float64), **HOG_PARAMS)\n",
        "        F = dummy.size\n",
        "        return np.empty((0, F), np.float32), np.empty((0,), np.int64)\n",
        "\n",
        "    X = np.concatenate(X_list, axis=0).astype(np.float32)\n",
        "    y = np.concatenate(y_list, axis=0).astype(np.int64)\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def _iter_batches(dl, label_mode=\"plant\"):\n",
        "    \"\"\"\n",
        "    Yield (xb, yb) as numpy arrays:\n",
        "      xb -> (B,H,W,C?), yb -> (B,)\n",
        "    \"\"\"\n",
        "    for batch in dl:\n",
        "        xb, yb = _unwrap_example(batch)\n",
        "        xb_np = _to_numpy(xb)\n",
        "        # Normalize image layout to (B,H,W,C?)\n",
        "        if xb_np.ndim == 4 and (xb_np.shape[1] in (1,3) and xb_np.shape[1] < xb_np.shape[-1]):\n",
        "            xb_np = np.transpose(xb_np, (0, 2, 3, 1))\n",
        "        elif xb_np.ndim == 3:\n",
        "            xb_np = xb_np[None, ...]\n",
        "\n",
        "        # Get (B,) labels robustly\n",
        "        yb_np = _labels_from_any(yb, label_mode)\n",
        "\n",
        "        # Final sanity\n",
        "        if xb_np.ndim != 4 or yb_np.ndim != 1 or xb_np.shape[0] != yb_np.shape[0]:\n",
        "            # Skip malformed batch\n",
        "            continue\n",
        "\n",
        "        yield xb_np, yb_np\n",
        "\n",
        "\n",
        "def eval_stream(ds, pca, clf, name=\"split\", label_mode=\"plant\"):\n",
        "    \"\"\"\n",
        "    Evaluate split by streaming batches: HOG -> PCA -> predict.\n",
        "    Prints metrics and returns (acc, f1_macro, f1_weighted).\n",
        "    \"\"\"\n",
        "    y_true_all, y_pred_all = [], []\n",
        "    for xb, yb in _iter_batches(ds, label_mode=label_mode):\n",
        "        Xb = batch_hog(xb)\n",
        "        if Xb.shape[0] == 0:\n",
        "            continue\n",
        "        preds = clf.predict(pca.transform(Xb)).astype(np.int64)\n",
        "        y_true_all.append(yb)\n",
        "        y_pred_all.append(preds)\n",
        "\n",
        "    if not y_true_all:\n",
        "        print(f\"[{name}] no data\")\n",
        "        return 0.0, 0.0, 0.0\n",
        "\n",
        "    y_true = np.concatenate(y_true_all)\n",
        "    y_pred = np.concatenate(y_pred_all)\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1m = f1_score(y_true, y_pred, average='macro',    zero_division=0)\n",
        "    f1w = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "    print(f\"[{name}] acc={acc:.4f}  f1_macro={f1m:.4f}  f1_weighted={f1w:.4f}\")\n",
        "    print(classification_report(y_true, y_pred, digits=3))\n",
        "    return acc, f1m, f1w\n",
        "\n",
        "def plot_confmat(y_true, y_pred, class_names, title, normalize='true'):\n",
        "    \"\"\"Utility to plot a confusion matrix.\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred, normalize=normalize)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "    fig, ax = plt.subplots(figsize=(6.5,6))\n",
        "    disp.plot(ax=ax, cmap='Blues', colorbar=True, values_format='.2f' if normalize else 'd')\n",
        "    ax.set_title(title); plt.tight_layout(); plt.show()\n",
        "\n",
        "def plot_f1_bars(y_true, y_pred, class_names, title):\n",
        "    \"\"\"Per-class F1 bar plot.\"\"\"\n",
        "    rep = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
        "    f1s, labels = [], []\n",
        "    for i, name in enumerate(class_names):\n",
        "        key = str(i)\n",
        "        if key in rep:\n",
        "            f1s.append(rep[key]['f1-score']); labels.append(name)\n",
        "    fig, ax = plt.subplots(figsize=(10,4.2))\n",
        "    ax.bar(np.arange(len(f1s)), f1s)\n",
        "    ax.set_xticks(np.arange(len(f1s))); ax.set_xticklabels(labels, rotation=45, ha='right')\n",
        "    ax.set_ylim(0,1.0); ax.set_ylabel(\"F1\"); ax.set_title(title)\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Main runner (legacy signature)\n",
        "# ----------------------------\n",
        "def make_loaders(label_mode=\"plant\", augment_train=False):\n",
        "    \"\"\"\n",
        "    Legacy entrypoint kept for compatibility with your notebook.\n",
        "    Expects the following globals to exist:\n",
        "      - train_ds : training dataset or loader\n",
        "      - val_ds   : validation dataset or loader (optional)\n",
        "      - test_ds  : test dataset or loader (optional)\n",
        "\n",
        "    If val_ds/test_ds are missing and train_ds is indexable, we will split it.\n",
        "    \"\"\"\n",
        "    # Resolve datasets/loaders from globals\n",
        "    if 'train_ds' not in globals():\n",
        "        raise RuntimeError(\"Global 'train_ds' is required. Please define train_ds before calling make_loaders().\")\n",
        "\n",
        "    local_train = globals()['train_ds']\n",
        "    local_val   = globals().get('val_ds', None)\n",
        "    local_test  = globals().get('test_ds', None)\n",
        "\n",
        "    # If val/test not provided: attempt to split\n",
        "    if (local_val is None or local_test is None) and _is_indexable_dataset(local_train):\n",
        "        local_train, local_val, local_test = _split_dataset(local_train, val_frac=0.15, test_frac=0.15, seed=RANDOM_SEED)\n",
        "    elif local_val is None or local_test is None:\n",
        "        raise RuntimeError(\"val_ds/test_ds not found. Provide them or make train_ds indexable so we can split.\")\n",
        "\n",
        "    # 1) Subsample train for HOG+PCA fit + training\n",
        "    X_train_raw, y_train = collect_subset_hog(local_train, TRAIN_FRACTION, rng, label_mode=label_mode)\n",
        "    if X_train_raw.shape[0] == 0:\n",
        "        raise RuntimeError(\"No training samples collected. Ensure your train_ds yields (image,label) and that TRAIN_FRACTION is > 0.\")\n",
        "\n",
        "    # 2) PCA on train subset\n",
        "    pca = PCA(n_components=N_COMPONENTS, svd_solver='randomized', random_state=RANDOM_SEED)\n",
        "    X_train_pca = pca.fit_transform(X_train_raw.astype(np.float32))\n",
        "\n",
        "    # 3) Train baseline models\n",
        "    lr = LogisticRegression(\n",
        "        multi_class='multinomial', solver='saga', max_iter=2000, n_jobs=-1,\n",
        "        class_weight=class_weights, verbose=0\n",
        "    ).fit(X_train_pca, y_train)\n",
        "\n",
        "    rf = None\n",
        "    if USE_RANDOM_FOREST:\n",
        "        rf = RandomForestClassifier(\n",
        "            n_estimators=400, max_depth=None, n_jobs=-1,\n",
        "            class_weight=class_weights, random_state=RANDOM_SEED\n",
        "        ).fit(X_train_pca, y_train)\n",
        "\n",
        "    # 4) Evaluate\n",
        "    lr_val_acc,  lr_val_f1m,  lr_val_f1w = eval_stream(local_val,  pca, lr, \"VAL / LogReg\",  label_mode=label_mode)\n",
        "    lr_test_acc, lr_test_f1m, lr_test_f1w = eval_stream(local_test, pca, lr, \"TEST / LogReg\", label_mode=label_mode)\n",
        "\n",
        "    if rf is not None:\n",
        "        rf_val_acc,  rf_val_f1m,  rf_val_f1w = eval_stream(local_val,  pca, rf, \"VAL / RF\",  label_mode=label_mode)\n",
        "        rf_test_acc, rf_test_f1m, rf_test_f1w = eval_stream(local_test, pca, rf, \"TEST / RF\", label_mode=label_mode)\n",
        "\n",
        "    # You can optionally return more (e.g., pca, models) if you want.\n",
        "    return lr_test_f1m, lr_test_f1w\n",
        "\n",
        "\n",
        "# ============================\n",
        "# Example calls (keep as-is):\n",
        "# ============================\n",
        "# You must already have these in your environment:\n",
        "#   train_ds, val_ds, test_ds\n",
        "# If you don't have val_ds/test_ds, remove them and the code will split train_ds automatically.\n",
        "\n",
        "print(\"Classic_plant\")\n",
        "classic_plant = make_loaders(label_mode=\"plant\")\n",
        "\n",
        "print(\"Classic_disease\")\n",
        "classic_disease = make_loaders(label_mode=\"disease\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "print(inspect.signature(make_loaders))\n"
      ],
      "metadata": {
        "id": "Sp59AIzl58vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oxaNTLj6mNX"
      },
      "source": [
        "Both Classical model show bed result in both cases. In disisise clasification the do not usefull at all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fTQ6PN4PAqA"
      },
      "source": [
        "##Second cheking og edgeliks by dublikates ore sami dublicater recognithion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBqne1362wjs"
      },
      "source": [
        "Exact duplicates by hash (MD5/SHA1) between splits, heare we chek if we have same sort of edge leak"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfMeO2ycfwjC"
      },
      "outputs": [],
      "source": [
        "# === Duplicate image scan (MD5) adapted to your project layout ===\n",
        "from pathlib import Path\n",
        "import hashlib, os\n",
        "from collections import defaultdict\n",
        "\n",
        "# Reuse your ROOT and EXTS if they exist; otherwise set safe defaults\n",
        "try:\n",
        "    _ROOT = ROOT\n",
        "except NameError:\n",
        "    _ROOT = Path(\".\")\n",
        "try:\n",
        "    _EXTS = EXTS\n",
        "except NameError:\n",
        "    _EXTS = {'.jpg','.jpeg','.png','.bmp','.webp','.tif','.tiff'}\n",
        "\n",
        "def file_md5(path: Path, bufsize: int = 1 << 20) -> str:\n",
        "    \"\"\"Compute MD5 for a file in streaming mode (1MB chunks).\"\"\"\n",
        "    h = hashlib.md5()\n",
        "    with open(path, 'rb') as f:\n",
        "        for chunk in iter(lambda: f.read(bufsize), b''):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def _detect_split(path: Path) -> str | None:\n",
        "    \"\"\"Return 'train'/'validation'/'test' if path contains such a segment, else None.\"\"\"\n",
        "    parts = {p.lower() for p in path.parts}\n",
        "    for s in (\"train\", \"validation\", \"test\"):\n",
        "        if s in parts:\n",
        "            return s\n",
        "    return None\n",
        "\n",
        "def _is_image(path: Path, exts: set[str]) -> bool:\n",
        "    \"\"\"Check by extension (case-insensitive).\"\"\"\n",
        "    return path.is_file() and path.suffix.lower() in exts\n",
        "\n",
        "def iter_image_files(root: Path, exts: set[str]) -> list[Path]:\n",
        "    \"\"\"Yield all image files under train/validation/test (deep).\"\"\"\n",
        "    out = []\n",
        "    for split in (\"train\", \"validation\", \"test\"):\n",
        "        d = Path(root) / split\n",
        "        if not d.exists():\n",
        "            continue\n",
        "        # Walk recursively; avoid hidden dirs for small speed-up\n",
        "        for cur, dirnames, files in os.walk(d):\n",
        "            # prune hidden dirs\n",
        "            dirnames[:] = [nm for nm in dirnames if not nm.startswith(\".\")]\n",
        "            for fn in files:\n",
        "                if fn.startswith(\".\"):  # skip hidden files\n",
        "                    continue\n",
        "                p = Path(cur) / fn\n",
        "                if _is_image(p, exts):\n",
        "                    out.append(p)\n",
        "    return out\n",
        "\n",
        "def scan_hashes_root(root: Path = _ROOT, exts: set[str] = _EXTS):\n",
        "    \"\"\"\n",
        "    Scan all images under ROOT/{train,validation,test}, compute MD5, and\n",
        "    return (hash_map, dup_groups, cross_split_groups).\n",
        "\n",
        "    - hash_map: {md5: [absolute_path, ...]}\n",
        "    - dup_groups: {md5: [paths...]} only hashes that appear >1 time\n",
        "    - cross_split_groups: subset of dup_groups where duplicates span different splits\n",
        "    \"\"\"\n",
        "    hash_map: dict[str, list[str]] = defaultdict(list)\n",
        "\n",
        "    files = iter_image_files(root, exts)\n",
        "    for p in files:\n",
        "        try:\n",
        "            md5 = file_md5(p)\n",
        "            hash_map[md5].append(str(p))\n",
        "        except Exception as e:\n",
        "            # Skip unreadable files but keep going\n",
        "            print(f\"[warn] Failed {p}: {e}\")\n",
        "\n",
        "    dup_groups = {k: v for k, v in hash_map.items() if len(v) > 1}\n",
        "\n",
        "    def _splits_for_group(paths: list[str]) -> set[str]:\n",
        "        ss = set()\n",
        "        for s in paths:\n",
        "            sp = _detect_split(Path(s))\n",
        "            if sp is not None:\n",
        "                ss.add(sp)\n",
        "        return ss\n",
        "\n",
        "    cross_split_groups = {\n",
        "        k: v for k, v in dup_groups.items()\n",
        "        if len(_splits_for_group(v)) >= 2\n",
        "    }\n",
        "\n",
        "    # Compact summary print\n",
        "    print(f\"Scanned files: {len(files)}\")\n",
        "    print(f\"Exact duplicate groups: {len(dup_groups)}\")\n",
        "    print(f\"Cross-split duplicate groups: {len(cross_split_groups)}\")\n",
        "\n",
        "    # Optional: show a tiny preview of first few groups\n",
        "    preview = 3\n",
        "    if dup_groups and preview:\n",
        "        print(\"\\nSample duplicate groups (up to 3):\")\n",
        "        for i, (h, paths) in enumerate(list(dup_groups.items())[:preview], 1):\n",
        "            print(f\"[{i}] md5={h} ({len(paths)} files)\")\n",
        "            for p in paths[:4]:\n",
        "                print(\"   \", p)\n",
        "            if len(paths) > 4:\n",
        "                print(\"    ...\")\n",
        "\n",
        "    return hash_map, dup_groups, cross_split_groups\n",
        "\n",
        "# --- Usage example (keeps your variables) ---\n",
        "hmap, dups, cross = scan_hashes_root(ROOT)\n",
        "print(\"Exact dup groups:\", len(dups))\n",
        "print(\"Cross-split dup groups:\", len(cross))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPTY9m4zhkwm"
      },
      "source": [
        "\n",
        "\n",
        "> Near-duplicates (perceptual hash, pHash/aHash/dHash)\n",
        "This helps catch crops, resizes, and minor changes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLW_reCWhhAC"
      },
      "outputs": [],
      "source": [
        "# === Perceptual near-duplicate scan (pHash) for your project layout ===\n",
        "# - Uses existing ROOT/EXTS if present\n",
        "# - Indexes images under ROOT/{train,validation,test}\n",
        "# - Computes perceptual hash (pHash) and finds cross-split near-duplicates by Hamming distance\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "from typing import Iterable, Tuple, List\n",
        "from PIL import Image\n",
        "import imagehash\n",
        "\n",
        "# Reuse your ROOT and EXTS if available; otherwise set safe defaults\n",
        "try:\n",
        "    _ROOT = ROOT\n",
        "except NameError:\n",
        "    _ROOT = Path(\".\")\n",
        "try:\n",
        "    _EXTS = EXTS\n",
        "except NameError:\n",
        "    _EXTS = {'.jpg','.jpeg','.png','.bmp','.webp','.tif','.tiff'}\n",
        "\n",
        "# ----------------- helpers -----------------\n",
        "def _is_image(path: Path, exts: set) -> bool:\n",
        "    \"\"\"Basic extension check (case-insensitive).\"\"\"\n",
        "    return path.is_file() and path.suffix.lower() in exts\n",
        "\n",
        "def _detect_split(path: Path) -> str | None:\n",
        "    \"\"\"Return the split name ('train'|'validation'|'test') if present in the path parts.\"\"\"\n",
        "    parts = [p.lower() for p in path.parts]\n",
        "    for s in (\"train\",\"validation\",\"test\"):\n",
        "        if s in parts:\n",
        "            return s\n",
        "    return None\n",
        "\n",
        "def _iter_image_files(root: Path, splits: Iterable[str] = (\"train\",\"validation\",\"test\"),\n",
        "                      exts: set = _EXTS) -> List[Path]:\n",
        "    \"\"\"Yield all image files from selected splits recursively.\"\"\"\n",
        "    out = []\n",
        "    for split in splits:\n",
        "        d = Path(root) / split\n",
        "        if not d.exists():\n",
        "            continue\n",
        "        for cur, dirnames, files in os.walk(d):\n",
        "            # prune hidden dirs\n",
        "            dirnames[:] = [nm for nm in dirnames if not nm.startswith(\".\")]\n",
        "            for fn in files:\n",
        "                if fn.startswith(\".\"):\n",
        "                    continue\n",
        "                p = Path(cur) / fn\n",
        "                if _is_image(p, exts):\n",
        "                    out.append(p)\n",
        "    return out\n",
        "\n",
        "# ----------------- indexing (pHash) -----------------\n",
        "def phash_index(root: Path,\n",
        "                splits: Iterable[str] = (\"train\",\"validation\",\"test\"),\n",
        "                hash_size: int = 16) -> List[Tuple[str, str, imagehash.ImageHash]]:\n",
        "    \"\"\"\n",
        "    Build an index of (split, path_str, phash).\n",
        "    - hash_size=16 → 16x16 DCT (larger → more precise, slightly slower)\n",
        "    \"\"\"\n",
        "    files = _iter_image_files(root, splits=splits, exts=_EXTS)\n",
        "    idx: List[Tuple[str, str, imagehash.ImageHash]] = []\n",
        "    for p in files:\n",
        "        split = _detect_split(p)\n",
        "        if split is None:\n",
        "            continue\n",
        "        try:\n",
        "            with Image.open(p) as im:\n",
        "                # Convert to RGB to normalize mode differences\n",
        "                h = imagehash.phash(im.convert(\"RGB\"), hash_size=hash_size)\n",
        "            idx.append((split, str(p), h))\n",
        "        except Exception as e:\n",
        "            # Keep going on errors (corrupted/unreadable files, etc.)\n",
        "            print(f\"[warn] skip {p}: {e}\")\n",
        "            continue\n",
        "    return idx\n",
        "\n",
        "# ----------------- near-duplicate detection -----------------\n",
        "def near_dups(idx: List[Tuple[str,str,imagehash.ImageHash]],\n",
        "              max_dist: int = 6,\n",
        "              cross_split_only: bool = True,\n",
        "              prefix_bucket: int = 4) -> List[Tuple[str, str, int]]:\n",
        "    \"\"\"\n",
        "    Find perceptual near-duplicate pairs:\n",
        "    - max_dist: maximum Hamming distance between hashes to consider as near-duplicate\n",
        "    - cross_split_only: if True, keep only pairs that span different splits\n",
        "    - prefix_bucket: compare only items that share the same first N hex chars (speed-up)\n",
        "    Returns list of (path_i, path_j, distance).\n",
        "    \"\"\"\n",
        "    # Bucket by first N hex chars to cut down comparisons\n",
        "    buckets: dict[str, list[Tuple[str,str,imagehash.ImageHash]]] = {}\n",
        "    for split, path, h in idx:\n",
        "        key = str(h)[:prefix_bucket] if prefix_bucket > 0 else str(h)\n",
        "        buckets.setdefault(key, []).append((split, path, h))\n",
        "\n",
        "    pairs: List[Tuple[str, str, int]] = []\n",
        "    for key, items in buckets.items():\n",
        "        n = len(items)\n",
        "        if n < 2:\n",
        "            continue\n",
        "        # O(n^2) inside each small bucket\n",
        "        for i in range(n):\n",
        "            si, pi, hi = items[i]\n",
        "            for j in range(i+1, n):\n",
        "                sj, pj, hj = items[j]\n",
        "                d = int(hi - hj)  # Hamming distance for imagehash.ImageHash\n",
        "                if d <= max_dist:\n",
        "                    if not cross_split_only or (si != sj):\n",
        "                        pairs.append((pi, pj, d))\n",
        "    return pairs\n",
        "\n",
        "# ----------------- run -----------------\n",
        "# Usage:\n",
        "#   idx = phash_index(_ROOT, splits=(\"train\",\"validation\",\"test\"), hash_size=16)\n",
        "#   pairs = near_dups(idx, max_dist=6, cross_split_only=True, prefix_bucket=4)\n",
        "#   print(\"Cross-split near-duplicate pairs:\", len(pairs))\n",
        "#   for a,b,d in pairs[:10]:\n",
        "#       print(d, \"\\n \", a, \"\\n \", b)\n",
        "\n",
        "idx = phash_index(_ROOT, splits=(\"train\",\"validation\",\"test\"), hash_size=16)\n",
        "pairs = near_dups(idx, max_dist=6, cross_split_only=True, prefix_bucket=4)\n",
        "\n",
        "print(\"Cross-split near-duplicate pairs:\", len(pairs))\n",
        "for a, b, d in pairs[:10]:\n",
        "    print(d, \"\\n \", a, \"\\n \", b)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3ZHiwQDCQ9o"
      },
      "source": [
        "##Models Compitishion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfNxnydQ53XU"
      },
      "source": [
        "Now we'll check other models, to compare our own with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSXWfLtM4npW"
      },
      "outputs": [],
      "source": [
        "# ==========================================================\n",
        "# Fair sweep on 20% TRAIN subset (no disk saves)\n",
        "# - Works with label_mode in {\"plant\",\"disease\"}\n",
        "# - Plug-in metrics (your own metrics can be passed in)\n",
        "# - Supports your class weight dicts {class_name: weight}\n",
        "# - Uses prepared splits at ROOT/train|validation|test\n",
        "# - TF-like per-image standardization (on 0..255 scale)\n",
        "# - Auto image size per model (fixes DeiT 224 vs 256)\n",
        "# - Colab-safe DataLoader (num_workers=0) to avoid MP assertion\n",
        "# ==========================================================\n",
        "\n",
        "# If needed in your environment:\n",
        "# !pip -q install timm\n",
        "\n",
        "from pathlib import Path\n",
        "import os, gc, time, math, random, numpy as np, warnings\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "import timm\n",
        "\n",
        "# -----------------------\n",
        "# Config\n",
        "# -----------------------\n",
        "ROOT             = Path(data_path) / \"image data\" if (Path(data_path) / \"image data\").exists() else Path(data_path)\n",
        "SUBSAMPLE_FRAC   = 0.10         # exact 20% of TRAIN\n",
        "EPOCHS_SWEEP     = 4\n",
        "BATCH_SIZE       = 64\n",
        "NUM_WORKERS_SAFE = 0            # avoids Colab \"can only test a child process\" assertion\n",
        "PIN_MEM          = (torch.cuda.is_available())\n",
        "LR               = 3e-4\n",
        "WEIGHT_DECAY     = 1e-4\n",
        "RANDOM_SEED      = 42\n",
        "\n",
        "# Strategy for CE weights when houdini_loss is NOT provided:\n",
        "USE_TRAIN_BALANCED_WEIGHTS = False  # if False -> prefer your dict weights when provided\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "g = torch.Generator().manual_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED); np.random.seed(RANDOM_SEED); torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "print(\"Device:\", device)\n",
        "print(\"Root:\", str(ROOT))\n",
        "\n",
        "# -----------------------\n",
        "# TF-like per-image standardization (on 0..255 scale)\n",
        "# -----------------------\n",
        "def per_image_standardization_torch(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    x: float tensor in [0,1], shape [3,H,W].\n",
        "    Emulates tf.image.per_image_standardization(image * 255.0):\n",
        "      - mean/std over ALL pixels and channels on 0..255 scale\n",
        "      - adjusted_std = max(std, 1/sqrt(N))\n",
        "      - output = (x*255 - mean) / adjusted_std\n",
        "    \"\"\"\n",
        "    x255 = x * 255.0\n",
        "    mean = x255.mean()\n",
        "    var  = x255.var(unbiased=False)\n",
        "    std  = torch.sqrt(var + 1e-12)\n",
        "    adjusted_std = torch.maximum(std, torch.tensor(1.0 / math.sqrt(x255.numel()), device=x.device, dtype=x.dtype))\n",
        "    return (x255 - mean) / adjusted_std\n",
        "\n",
        "class PerImageStandardize(object):\n",
        "    def __call__(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return per_image_standardization_torch(x)\n",
        "\n",
        "def build_transforms(img_size: int):\n",
        "    \"\"\"Build train/eval transforms for a given model's required img_size.\"\"\"\n",
        "    train_tfms = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ColorJitter(0.2,0.2,0.2,0.03),\n",
        "        transforms.ToTensor(),\n",
        "        PerImageStandardize(),  # TF-like standardization\n",
        "    ])\n",
        "    eval_tfms = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "        transforms.ToTensor(),\n",
        "        PerImageStandardize(),\n",
        "    ])\n",
        "    return train_tfms, eval_tfms\n",
        "\n",
        "# -----------------------\n",
        "# Dataset that supports label_mode={\"plant\",\"disease\"}\n",
        "# Expected tree: ROOT/split/PLANT/DISEASE/*.jpg\n",
        "# -----------------------\n",
        "EXTS = {'.jpg','.jpeg','.png','.bmp','.webp','.tif','.tiff'}\n",
        "\n",
        "def collect_split(split, label_mode=\"plant\"):\n",
        "    \"\"\"\n",
        "    Build (paths, labels_str) for split.\n",
        "    label_mode='plant'   -> label = plant_dir.name\n",
        "    label_mode='disease' -> label = disease_dir.name (merged across plants)\n",
        "    \"\"\"\n",
        "    assert label_mode in {\"plant\",\"disease\"}\n",
        "    paths, labels = [], []\n",
        "    split_dir = ROOT / split\n",
        "    if not split_dir.exists():\n",
        "        return paths, labels\n",
        "    for plant_dir in sorted(p for p in split_dir.iterdir() if p.is_dir()):\n",
        "        plant = plant_dir.name\n",
        "        for disease_dir in sorted(p for p in plant_dir.iterdir() if p.is_dir()):\n",
        "            disease = disease_dir.name\n",
        "            for f in disease_dir.rglob(\"*\"):\n",
        "                if f.is_file() and f.suffix.lower() in EXTS:\n",
        "                    paths.append(str(f))\n",
        "                    labels.append(plant if label_mode==\"plant\" else disease)\n",
        "    return paths, labels\n",
        "\n",
        "class PathsDataset(Dataset):\n",
        "    \"\"\"Lightweight dataset built from arrays of paths and numeric labels.\"\"\"\n",
        "    def __init__(self, paths_np, labels_np, transform):\n",
        "        self.paths_np = paths_np\n",
        "        self.labels_np = labels_np\n",
        "        self.transform = transform\n",
        "    def __len__(self): return len(self.paths_np)\n",
        "    def __getitem__(self,i):\n",
        "        p = self.paths_np[i]; y = int(self.labels_np[i])\n",
        "        img = Image.open(p).convert(\"RGB\")\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        # transform ends with ToTensor + PerImageStandardize, so 'img' is torch.Tensor\n",
        "        return img, y\n",
        "\n",
        "# -----------------------\n",
        "# Build base label space and stratified subset for TRAIN\n",
        "# -----------------------\n",
        "def make_base_and_subset(label_mode=\"plant\"):\n",
        "    \"\"\"Collect splits, build class->index mapping, and create 20% stratified subset indices.\"\"\"\n",
        "    tr_p, tr_l_s = collect_split(\"train\", label_mode)\n",
        "    va_p, va_l_s = collect_split(\"validation\", label_mode)\n",
        "    te_p, te_l_s = collect_split(\"test\", label_mode)\n",
        "\n",
        "    classes = sorted(set(tr_l_s + va_l_s + te_l_s))\n",
        "    lbl2idx = {c:i for i,c in enumerate(classes)}\n",
        "\n",
        "    tr_y = np.array([lbl2idx[s] for s in tr_l_s], dtype=np.int64)\n",
        "    va_y = np.array([lbl2idx[s] for s in va_l_s], dtype=np.int64)\n",
        "    te_y = np.array([lbl2idx[s] for s in te_l_s], dtype=np.int64)\n",
        "\n",
        "    tr_p = np.array(tr_p); va_p = np.array(va_p); te_p = np.array(te_p)\n",
        "\n",
        "    # build stratified 20% from TRAIN ONLY\n",
        "    by_c = {}\n",
        "    for i,t in enumerate(tr_y): by_c.setdefault(int(t), []).append(i)\n",
        "    rng = np.random.default_rng(RANDOM_SEED)\n",
        "    subset_idx = []\n",
        "    for idxs in by_c.values():\n",
        "        k = max(1, int(round(len(idxs)*SUBSAMPLE_FRAC)))\n",
        "        subset_idx.extend(rng.choice(idxs, size=k, replace=False).tolist())\n",
        "    rng.shuffle(subset_idx)\n",
        "\n",
        "    base = dict(\n",
        "        classes=classes, C=len(classes),\n",
        "        train_paths=tr_p, train_targets=tr_y,\n",
        "        val_paths=va_p,   val_targets=va_y,\n",
        "        test_paths=te_p,  test_targets=te_y,\n",
        "        train_subset_idx=subset_idx\n",
        "    )\n",
        "    return base\n",
        "\n",
        "# -----------------------\n",
        "# Rebuild DataLoaders for a given img_size (per model)\n",
        "# -----------------------\n",
        "def build_dls_for_imgsize(img_size, base):\n",
        "    tr_tfms, ev_tfms = build_transforms(img_size)\n",
        "\n",
        "    train_full = PathsDataset(base['train_paths'], base['train_targets'], transform=tr_tfms)\n",
        "    val_full   = PathsDataset(base['val_paths'],   base['val_targets'],   transform=ev_tfms)\n",
        "    test_full  = PathsDataset(base['test_paths'],  base['test_targets'],  transform=ev_tfms)\n",
        "\n",
        "    train_ds = Subset(train_full, base['train_subset_idx'])\n",
        "    val_ds   = val_full\n",
        "    test_ds  = test_full\n",
        "\n",
        "    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=NUM_WORKERS_SAFE, pin_memory=PIN_MEM, generator=g)\n",
        "    val_dl   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
        "                          num_workers=NUM_WORKERS_SAFE, pin_memory=PIN_MEM)\n",
        "    test_dl  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n",
        "                          num_workers=NUM_WORKERS_SAFE, pin_memory=PIN_MEM)\n",
        "    return train_dl, val_dl, test_dl\n",
        "\n",
        "def infer_img_size_from_model(m):\n",
        "    \"\"\"Read target input size from timm model default_cfg; fallback to 224.\"\"\"\n",
        "    cfg = getattr(m, 'default_cfg', None) or {}\n",
        "    inp = cfg.get('input_size', (3, 224, 224))\n",
        "    return int(inp[1])\n",
        "\n",
        "# -----------------------\n",
        "# Weights alignment and criterion builder\n",
        "# -----------------------\n",
        "def align_weights_from_dict(classes, weights_dict:dict|None):\n",
        "    \"\"\"\n",
        "    Align a user-provided dict {class_name: weight} to a torch tensor in `classes` order.\n",
        "    Warn if there are missing or extra keys. If missing, fallback to 1.0.\n",
        "    \"\"\"\n",
        "    if not weights_dict:\n",
        "        return None\n",
        "    miss = [c for c in classes if c not in weights_dict]\n",
        "    extra = [k for k in weights_dict.keys() if k not in classes]\n",
        "    if miss:\n",
        "        warnings.warn(f\"[class weights] missing classes: {miss}; using 1.0 for them\")\n",
        "    if extra:\n",
        "        warnings.warn(f\"[class weights] extra keys not in classes: {extra} (ignored)\")\n",
        "    arr = np.array([weights_dict.get(c, 1.0) for c in classes], dtype=np.float32)\n",
        "    return torch.tensor(arr, device=device)\n",
        "\n",
        "def build_ce_balanced_weights_from_subset(targets_full, subset_idx, C):\n",
        "    \"\"\"Compute classic balanced weights from TRAIN subset labels.\"\"\"\n",
        "    sub_targets = targets_full[subset_idx]\n",
        "    counts = np.bincount(sub_targets, minlength=C).astype(np.float32)\n",
        "    class_w = (counts.sum() / (C * np.clip(counts, 1, None))).astype(np.float32)\n",
        "    return torch.tensor(class_w, device=device)\n",
        "\n",
        "# -----------------------\n",
        "# Eval helper: returns (loss, acc, f1_macro) + y_true/y_pred/y_proba\n",
        "# -----------------------\n",
        "@torch.no_grad()\n",
        "def eval_split(model, dl, criterion):\n",
        "    model.eval(); y_true = []; y_pred = []; y_prob = []; loss_sum = 0.0; n_tot = 0\n",
        "    for x,y in dl:\n",
        "        x,y = x.to(device), y.to(device)\n",
        "        logits = model(x); loss = criterion(logits, y)\n",
        "        loss_sum += loss.item()*y.size(0); n_tot += y.size(0)\n",
        "        y_true.extend(y.cpu().numpy().tolist())\n",
        "        p = logits.argmax(1)\n",
        "        y_pred.extend(p.cpu().numpy().tolist())\n",
        "        y_prob.append(torch.softmax(logits, dim=1).cpu().numpy())\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1m = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    y_prob = np.concatenate(y_prob, axis=0) if len(y_prob) else None\n",
        "    return (loss_sum/n_tot if n_tot>0 else 0.0), acc, f1m, np.array(y_true), np.array(y_pred), y_prob\n",
        "\n",
        "# -----------------------\n",
        "# Train a timm model for the sweep (per-model img size)\n",
        "# -----------------------\n",
        "def train_model(name, base, classes, criterion_builder, epochs=EPOCHS_SWEEP, lr=LR, wd=WEIGHT_DECAY):\n",
        "    # free memory between models\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # create model first to get its suggested input size\n",
        "    m = timm.create_model(name, pretrained=True, num_classes=len(classes), drop_rate=0.2).to(device)\n",
        "    img_size = infer_img_size_from_model(m)\n",
        "\n",
        "    # dataloaders for this img_size\n",
        "    train_dl, val_dl, test_dl = build_dls_for_imgsize(img_size, base)\n",
        "\n",
        "    # criterion (closure over device/weights)\n",
        "    crit = criterion_builder(device)\n",
        "\n",
        "    # optional tweak for short-run stability\n",
        "    local_lr = lr\n",
        "    if 'convnext_tiny' in name:\n",
        "        local_lr = min(lr, 2e-4)\n",
        "\n",
        "    opt  = torch.optim.AdamW(m.parameters(), lr=local_lr, weight_decay=wd)\n",
        "    sch  = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n",
        "\n",
        "    best_f1 = -1.0; best_state = None\n",
        "    epoch_times = []\n",
        "    t0 = time.time()\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        m.train()\n",
        "        ep_start = time.time()\n",
        "        for x,y in train_dl:\n",
        "            x,y = x.to(device), y.to(device)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
        "                logits = m(x); loss = crit(logits, y)\n",
        "            scaler.scale(loss).backward()\n",
        "            # torch.nn.utils.clip_grad_norm_(m.parameters(), 1.0)  # uncomment if needed\n",
        "            scaler.step(opt); scaler.update()\n",
        "        sch.step()\n",
        "        ep_secs = time.time() - ep_start\n",
        "        epoch_times.append(ep_secs)\n",
        "\n",
        "        # validation metrics\n",
        "        v_loss, v_acc, v_f1, _, _, _ = eval_split(m, val_dl, crit)\n",
        "        if v_f1 > best_f1:\n",
        "            best_f1   = v_f1\n",
        "            best_state = {k: v.detach().cpu() for k,v in m.state_dict().items()}\n",
        "\n",
        "        print(f\"[{name}] epoch {ep:02d}/{epochs} | \"\n",
        "              f\"val acc {v_acc:.3f} f1M {v_f1:.3f} loss {v_loss:.3f} | \"\n",
        "              f\"time {ep_secs:.1f}s (img_size={img_size})\")\n",
        "\n",
        "    total_secs = time.time() - t0\n",
        "\n",
        "    # test with best state (in-memory only)\n",
        "    if best_state is not None:\n",
        "        m.load_state_dict(best_state, strict=True)\n",
        "\n",
        "    t_loss, t_acc, t_f1, y_true, y_pred, y_proba = eval_split(m, test_dl, crit)\n",
        "\n",
        "    params_M = sum(p.numel() for p in m.parameters())/1e6\n",
        "    return {\n",
        "        'name': name,\n",
        "        'val_f1_best': float(best_f1),\n",
        "        'test_acc': float(t_acc),\n",
        "        'test_f1': float(t_f1),\n",
        "        'params_M': float(params_M),\n",
        "        'total_secs': float(total_secs),\n",
        "        'avg_epoch_secs': float(np.mean(epoch_times)) if epoch_times else float('nan'),\n",
        "        'epoch_secs_list': [float(s) for s in epoch_times],\n",
        "        'y_true': y_true.tolist(),\n",
        "        'y_pred': y_pred.tolist(),\n",
        "        # y_proba can be large; include if you need calibration/AUC style metrics\n",
        "        # 'y_proba': y_proba.tolist() if y_proba is not None else None,\n",
        "    }, (m, crit, (train_dl, val_dl, test_dl))\n",
        "\n",
        "# -----------------------\n",
        "# Random Forest baseline on pooled features (optional)\n",
        "# -----------------------\n",
        "def random_forest_baseline(base, classes, backbone='resnet18'):\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "    # feature extractor @224\n",
        "    _, ev_tfms = build_transforms(224)\n",
        "    feat_train = PathsDataset(base['train_paths'][base['train_subset_idx']], base['train_targets'][base['train_subset_idx']], transform=ev_tfms)\n",
        "    feat_val   = PathsDataset(base['val_paths'],  base['val_targets'],  transform=ev_tfms)\n",
        "    feat_test  = PathsDataset(base['test_paths'], base['test_targets'], transform=ev_tfms)\n",
        "\n",
        "    dl_train = DataLoader(feat_train, batch_size=128, shuffle=False, num_workers=NUM_WORKERS_SAFE, pin_memory=PIN_MEM)\n",
        "    dl_val   = DataLoader(feat_val,   batch_size=128, shuffle=False, num_workers=NUM_WORKERS_SAFE, pin_memory=PIN_MEM)\n",
        "    dl_test  = DataLoader(feat_test,  batch_size=128, shuffle=False, num_workers=NUM_WORKERS_SAFE, pin_memory=PIN_MEM)\n",
        "\n",
        "    extractor = timm.create_model(backbone, pretrained=True, num_classes=0, global_pool='avg').to(device).eval()\n",
        "\n",
        "    def collect(dl):\n",
        "        feats, ys = [], []\n",
        "        with torch.no_grad():\n",
        "            for x,y in dl:\n",
        "                x = x.to(device)\n",
        "                f = extractor(x).detach().cpu().numpy()\n",
        "                feats.append(f); ys.extend(y.numpy().tolist())\n",
        "        return np.concatenate(feats, axis=0), np.array(ys, dtype=np.int64)\n",
        "\n",
        "    t0 = time.time()\n",
        "    Xtr, ytr = collect(dl_train)\n",
        "    Xva, yva = collect(dl_val)\n",
        "    Xte, yte = collect(dl_test)\n",
        "\n",
        "    rf = RandomForestClassifier(n_estimators=300, max_features='sqrt', n_jobs=-1, random_state=RANDOM_SEED)\n",
        "    rf.fit(Xtr, ytr)\n",
        "    secs = time.time() - t0\n",
        "\n",
        "    vpred = rf.predict(Xva); tpred = rf.predict(Xte)\n",
        "    vacc  = accuracy_score(yva, vpred)\n",
        "    tacc  = accuracy_score(yte, tpred)\n",
        "    tf1   = f1_score(yte, tpred, average='macro', zero_division=0)\n",
        "\n",
        "    return {\n",
        "        'name': f'random_forest({backbone}-feats)',\n",
        "        'val_f1_best': float('nan'),\n",
        "        'test_acc': float(tacc),\n",
        "        'test_f1': float(tf1),\n",
        "        'params_M': 0.0,\n",
        "        'total_secs': float(secs),\n",
        "        'avg_epoch_secs': float('nan'),\n",
        "        'epoch_secs_list': [],\n",
        "    }\n",
        "\n",
        "# -----------------------\n",
        "# Metrics plug-in examples\n",
        "# Each fn: (y_true, y_pred, y_proba, classes) -> dict\n",
        "# -----------------------\n",
        "def metric_confmat(y_true, y_pred, y_proba, classes):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=np.arange(len(classes)))\n",
        "    return {\"confusion_matrix\": cm.tolist()}\n",
        "\n",
        "def metric_report(y_true, y_pred, y_proba, classes):\n",
        "    rep = classification_report(y_true, y_pred, target_names=classes, zero_division=0, output_dict=True)\n",
        "    return {\"classification_report\": rep}  # nested dict is OK to log\n",
        "\n",
        "# Put your own custom metrics here (they will be called after each model test)\n",
        "metrics_list = [metric_confmat]  # extend with your metrics\n",
        "\n",
        "# -----------------------\n",
        "# Criterion builder factory\n",
        "# Uses your houdini_loss if present; else CrossEntropy with either your weights dict\n",
        "# or balanced weights computed from the TRAIN subset (configurable).\n",
        "# -----------------------\n",
        "def make_criterion_builder(base, classes, class_weights_dict=None):\n",
        "    aligned_user_w = align_weights_from_dict(classes, class_weights_dict) if class_weights_dict else None\n",
        "    ce_balanced_w  = build_ce_balanced_weights_from_subset(base['train_targets'], base['train_subset_idx'], len(classes))\n",
        "\n",
        "    if 'houdini_loss' in globals():\n",
        "        print(\"Using external houdini_loss.\")\n",
        "        def builder(_device):\n",
        "            def criterion(logits, y): return houdini_loss(logits, y)\n",
        "            return criterion\n",
        "        return builder\n",
        "    else:\n",
        "        if not USE_TRAIN_BALANCED_WEIGHTS and aligned_user_w is not None:\n",
        "            print(\"Using CrossEntropyLoss with YOUR aligned class weights.\")\n",
        "            def builder(_device):\n",
        "                return nn.CrossEntropyLoss(weight=aligned_user_w)\n",
        "            return builder\n",
        "        else:\n",
        "            print(\"Using CrossEntropyLoss with balanced class weights (computed on TRAIN subset).\")\n",
        "            def builder(_device):\n",
        "                return nn.CrossEntropyLoss(weight=ce_balanced_w)\n",
        "            return builder\n",
        "\n",
        "# -----------------------\n",
        "# Sweep runner (per label_mode)\n",
        "# -----------------------\n",
        "def run_sweep(label_mode=\"plant\", class_weights_dict=None, candidates=None, epochs=EPOCHS_SWEEP, add_rf=True):\n",
        "    base = make_base_and_subset(label_mode=label_mode)\n",
        "    classes = base['classes']\n",
        "    print(f\"Label mode: {label_mode} | Classes: {len(classes)} \"\n",
        "          f\"| Train={len(base['train_paths'])} | Val={len(base['val_paths'])} | Test={len(base['test_paths'])}\")\n",
        "    print(f\"Train subset: {len(base['train_subset_idx'])} ({SUBSAMPLE_FRAC*100:.0f}% of TRAIN)\")\n",
        "\n",
        "    if candidates is None:\n",
        "        candidates = [\n",
        "            'resnet18',\n",
        "            'resnet50',\n",
        "            'efficientnet_b2',\n",
        "            'convnext_tiny',\n",
        "            'deit_small_patch16_224',\n",
        "        ]\n",
        "\n",
        "    criterion_builder = make_criterion_builder(base, classes, class_weights_dict=class_weights_dict)\n",
        "\n",
        "    results = []\n",
        "    models_cache = {}\n",
        "\n",
        "    for name in candidates:\n",
        "        try:\n",
        "            r, pack = train_model(name, base, classes, criterion_builder, epochs=epochs, lr=LR, wd=WEIGHT_DECAY)\n",
        "            results.append(r)\n",
        "            models_cache[name] = pack  # (model, crit, (train_dl,val_dl,test_dl))\n",
        "            print(f\"{name:24s} best_val_f1={r['val_f1_best']:.3f} test_acc={r['test_acc']:.3f} \"\n",
        "                  f\"f1M={r['test_f1']:.3f} params={r['params_M']:.1f} \"\n",
        "                  f\"time={r['total_secs']:.1f}s (avg/epoch={r['avg_epoch_secs']:.1f}s)\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] {name}: {e}\")\n",
        "\n",
        "    if add_rf:\n",
        "        try:\n",
        "            rf_res = random_forest_baseline(base, classes, backbone='resnet18')\n",
        "            results.append(rf_res)\n",
        "            print(f\"{rf_res['name']:24s} test_acc={rf_res['test_acc']:.3f} \"\n",
        "                  f\"f1M={rf_res['test_f1']:.3f} time={rf_res['total_secs']:.1f}s\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] random_forest: {e}\")\n",
        "\n",
        "    # Sort by best validation F1; fallback to test_f1\n",
        "    def sort_key(d):\n",
        "        return (d.get('val_f1_best', float('-inf')), d.get('test_f1', float('-inf')))\n",
        "    results = sorted(results, key=sort_key, reverse=True)\n",
        "\n",
        "    # Run plug-in metrics on the BEST model (optional; you can loop over all if needed)\n",
        "    if metrics_list and len(models_cache):\n",
        "        best_name = results[0]['name']\n",
        "        model, crit, (tr_dl, va_dl, te_dl) = models_cache[best_name]\n",
        "        # Eval again to get arrays for metrics\n",
        "        _, _, _, y_true, y_pred, y_proba = eval_split(model, te_dl, crit)\n",
        "        extra = {}\n",
        "        for fn in metrics_list:\n",
        "            try:\n",
        "                extra |= fn(y_true, y_pred, y_proba, classes)\n",
        "            except Exception as e:\n",
        "                warnings.warn(f\"[metrics] '{getattr(fn,'__name__',str(fn))}' failed: {e}\")\n",
        "        results[0]['extra_metrics_test'] = extra\n",
        "\n",
        "        # Optional: print classification report for the best model\n",
        "        try:\n",
        "            rep = classification_report(y_true, y_pred, target_names=classes, zero_division=0)\n",
        "            print(\"\\n[Best model test classification report]\\n\", rep)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    sweep_out = {\n",
        "        'label_mode': label_mode,\n",
        "        'classes': classes,\n",
        "        'subset_sizes': {\n",
        "            'train_subset': len(base['train_subset_idx']),\n",
        "            'val': len(base['val_paths']),\n",
        "            'test': len(base['test_paths'])\n",
        "        },\n",
        "        'results': results\n",
        "    }\n",
        "\n",
        "    print(\"\\nTop-5:\")\n",
        "    for row in results[:5]:\n",
        "        print(row)\n",
        "    return sweep_out\n",
        "\n",
        "# -----------------------\n",
        "# HOW TO RUN\n",
        "# (Provide your dicts: class_weights_plants / class_weights_diseases)\n",
        "# Each dict must map CLASS NAME (as appears in 'classes') -> weight\n",
        "# -----------------------\n",
        "# Example:\n",
        "sweep_plants   = run_sweep(label_mode=\"plant\",   class_weights_dict=class_weights_plants,   candidates=None, epochs=EPOCHS_SWEEP, add_rf=True)\n",
        "sweep_diseases = run_sweep(label_mode=\"disease\", class_weights_dict=class_weights_disease, candidates=None, epochs=EPOCHS_SWEEP, add_rf=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aigyPn0958Aq"
      },
      "outputs": [],
      "source": [
        "# ===================== Sweep comparison plots (NO TRAINING) =====================\n",
        "# Expects:\n",
        "#   - sweep_plants   = run_sweep(... label_mode=\"plant\" ...)\n",
        "#   - sweep_diseases = run_sweep(... label_mode=\"disease\" ...)\n",
        "# Builds 4 bar charts per sweep (all models shown on each chart).\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def _extract_metric_list(sweep_out, key, default_key=None):\n",
        "    \"\"\"\n",
        "    Extracts (labels, values) for a metric from sweep_out['results'].\n",
        "    If 'key' does not exist in rows and default_key is provided, use it instead.\n",
        "    \"\"\"\n",
        "    rows = sweep_out.get('results', [])\n",
        "    names = [r.get('name', f'model_{i}') for i, r in enumerate(rows)]\n",
        "    # If metric not present, try default_key\n",
        "    if rows and key not in rows[0] and default_key is not None:\n",
        "        key = default_key\n",
        "    vals = [float(r.get(key, np.nan)) for r in rows]\n",
        "    return names, vals, key\n",
        "\n",
        "def _bar_plot(ax, names, vals, title, ylabel):\n",
        "    x = np.arange(len(names))\n",
        "    ax.bar(x, vals)\n",
        "    ax.set_title(title)\n",
        "    ax.set_ylabel(ylabel)\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(names, rotation=20, ha='right')\n",
        "    # Annotate bars\n",
        "    for xi, v in zip(x, vals):\n",
        "        label = \"NA\" if not np.isfinite(v) else f\"{v:.3f}\"\n",
        "        ax.text(xi, v if np.isfinite(v) else 0.0, label, ha='center', va='bottom', fontsize=8)\n",
        "    ax.grid(axis='y', alpha=0.25)\n",
        "\n",
        "def plot_sweep_four(sweep_out, title_prefix=\"\"):\n",
        "    \"\"\"\n",
        "    Makes 4 bar charts for a given sweep_out:\n",
        "      1) Validation F1 (best)\n",
        "      2) Validation Loss  (or Test F1 if val_loss not recorded)\n",
        "      3) Test Accuracy\n",
        "      4) Test F1\n",
        "    Shows all models on each chart.\n",
        "    \"\"\"\n",
        "    # 1) Validation F1 (best across epochs)\n",
        "    names1, vals1, _ = _extract_metric_list(sweep_out, \"val_f1_best\")\n",
        "\n",
        "    # 2) Validation Loss (fallback → test_f1)\n",
        "    names2, vals2, used2 = _extract_metric_list(sweep_out, \"val_loss_best\", default_key=\"test_f1\")\n",
        "    title2 = \"Validation Loss\" if used2 == \"val_loss_best\" else \"Test F1 (fallback)\"\n",
        "\n",
        "    # 3) Test Accuracy\n",
        "    names3, vals3, _ = _extract_metric_list(sweep_out, \"test_acc\")\n",
        "\n",
        "    # 4) Test F1\n",
        "    names4, vals4, _ = _extract_metric_list(sweep_out, \"test_f1\")\n",
        "\n",
        "    # Ensure consistent model ordering across plots (use order of results)\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "    plt.suptitle(f\"{title_prefix} — Model Comparison\", y=1.02, fontsize=14)\n",
        "\n",
        "    _bar_plot(axes[0,0], names1, vals1, \"Validation F1 (best)\", \"F1\")\n",
        "    _bar_plot(axes[0,1], names2, vals2, title2,            \"Loss\" if used2 == \"val_loss_best\" else \"F1\")\n",
        "    _bar_plot(axes[1,0], names3, vals3, \"Test Accuracy\",       \"Accuracy\")\n",
        "    _bar_plot(axes[1,1], names4, vals4, \"Test F1 (macro)\",     \"F1\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# --------- Run for both sweeps (each will show 4 charts with all 5 models) ----------\n",
        "plot_sweep_four(sweep_plants,   title_prefix=\"PLANT\")\n",
        "plot_sweep_four(sweep_diseases, title_prefix=\"DISEASE\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J8nT3cPCpSY"
      },
      "source": [
        "Plants desease\n",
        "most qualiti models heare is deit_small_patch16_224: F1≈0.994, acc≈0.996 and resnet50: F1≈0.994, acc≈0.995\n",
        "best balansed between qality and namber of parrams is efficientnet_b2: F1=0.992 прandи 7.7M params\n",
        "resnet18 that we used is fusters and with well qualiti and not to hight number of parms: F1+0.986 and 11M parms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbqd-OtwaYBR"
      },
      "source": [
        "#Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "124h-UIU3zMS"
      },
      "source": [
        "After model comnpitishion we diside to chose DeiT becouse that model the best result with the best condiant indicators. We also diside to use to loss function tigesser becose Houdini had much better loss that show better mattematishional result bat CE+Weights show better F1(F1=0.867 vs F1=0.844) that more usfull in real cases.\n",
        "\n",
        "How we tooned model:\n",
        "1. Learning Rate Schedulers:\n",
        "WarmRestarts:\n",
        " First of all we disede to stabalize ower few firs epoch becouse of that we diside increase learning rate for firs part of cicle. (It will help to model exit to local minimum and it vill hel to find better one)\n",
        " Seconde\n",
        "2. We also increase namber of Epoch and patience namber(Give more time for improovment)\n",
        "3. Gradient Clipping: limits the norm of gradients it help Predotvrashchayet gradient explosion prevents gradient explosion (When gradint of lost function start to grow exponentionally)\n",
        "4. DropPath turn of some neyronse its can help prevent overfit becouse the model less depent of specidic layers   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QL8SU3z2aa5A"
      },
      "outputs": [],
      "source": [
        "# === DeiT multi-task (plant, disease) with mixed loss (0.3*Houdini + 0.7*CEweights) ===\n",
        "# - Data tree: ROOT/split/PLANT/DISEASE/*.jpg\n",
        "# - Two heads: plant_head, disease_head\n",
        "# - Proper regularization: drop_path (DeiT), label smoothing, dropout in heads, grad clipping\n",
        "# - Per-image standardization (TF-like), no ImageNet normalize, as in your pipeline\n",
        "\n",
        "import os, time, math, warnings, numpy as np\n",
        "from pathlib import Path\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
        "import timm\n",
        "\n",
        "# -----------------------\n",
        "# Config\n",
        "# -----------------------\n",
        "ROOT             = Path(data_path) / \"image data\" if (Path(data_path) / \"image data\").exists() else Path(data_path)\n",
        "MODEL_NAME       = \"deit_small_mt_houdini0.3_cew0.7\"\n",
        "EPOCHS           = 20\n",
        "PATIENCE         = 4\n",
        "BATCH_SIZE       = 64\n",
        "NUM_WORKERS      = 2\n",
        "LR               = 3e-4\n",
        "WEIGHT_DECAY     = 1e-4\n",
        "LABEL_SMOOTHING  = 0.1\n",
        "SUBSAMPLE_FRAC   = 0.2          # set to 0.20 for 20% of train\n",
        "RANDOM_SEED      = 42\n",
        "TEST_EVERY_EPOCH = True\n",
        "DROPPATH_RATE    = 0.1          # stochastic depth inside DeiT\n",
        "HEAD_DROPOUT     = 0.2          # dropout before linear heads\n",
        "MAX_GRAD_NORM    = 1.0          # gradient clipping\n",
        "IMG_SIZE         = 224          # DeiT default\n",
        "\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "pin_mem = (device.type == 'cuda')\n",
        "print(\"Device:\", device)\n",
        "print(\"Root:\", str(ROOT))\n",
        "\n",
        "# -----------------------\n",
        "# Optional: your weights dicts (class_name -> weight)\n",
        "# Fill these from your code, or leave None to auto-balance on subset:\n",
        "# -----------------------\n",
        "class_weights_plants   = None  # e.g., {\"tomato\": 0.8, \"rice\": 1.2, ...}\n",
        "class_weights_diseases = None  # e.g., {\"late blight\": 1.3, ...}\n",
        "\n",
        "# -----------------------\n",
        "# TF-like per-image standardization (on 0..255 scale)\n",
        "# -----------------------\n",
        "def per_image_standardization_torch(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Standardize a single image tensor channel-wise after scaling to [0,255].\"\"\"\n",
        "    x255 = x * 255.0\n",
        "    mean = x255.mean()\n",
        "    var  = x255.var(unbiased=False)\n",
        "    std  = torch.sqrt(var + 1e-12)\n",
        "    adjusted_std = torch.maximum(std, torch.tensor(1.0 / math.sqrt(x255.numel()), device=x.device, dtype=x.dtype))\n",
        "    return (x255 - mean) / adjusted_std\n",
        "\n",
        "class PerImageStandardize(object):\n",
        "    def __call__(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return per_image_standardization_torch(x)\n",
        "\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE), interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(0.2,0.2,0.2,0.03),\n",
        "    transforms.ToTensor(),\n",
        "    PerImageStandardize(),\n",
        "])\n",
        "eval_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE), interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "    transforms.ToTensor(),\n",
        "    PerImageStandardize(),\n",
        "])\n",
        "\n",
        "# -----------------------\n",
        "# Build paths + multi-task labels from file tree\n",
        "# -----------------------\n",
        "EXTS = {'.jpg','.jpeg','.png','.bmp','.webp','.tif','.tiff'}\n",
        "\n",
        "def collect_split_mt(split: str) -> Tuple[List[str], List[str], List[str]]:\n",
        "    \"\"\"\n",
        "    Return (paths, plant_labels_str, disease_labels_str) for given split.\n",
        "    Expect: ROOT/split/PLANT/DISEASE/*.*\n",
        "    \"\"\"\n",
        "    split_dir = ROOT / split\n",
        "    paths, plants, diseases = [], [], []\n",
        "    if not split_dir.exists():\n",
        "        return paths, plants, diseases\n",
        "    for plant_dir in sorted(p for p in split_dir.iterdir() if p.is_dir()):\n",
        "        plant = plant_dir.name\n",
        "        for disease_dir in sorted(p for p in plant_dir.iterdir() if p.is_dir()):\n",
        "            disease = disease_dir.name\n",
        "            for f in disease_dir.rglob(\"*\"):\n",
        "                if f.is_file() and f.suffix.lower() in EXTS:\n",
        "                    paths.append(str(f)); plants.append(plant); diseases.append(disease)\n",
        "    return paths, plants, diseases\n",
        "\n",
        "def build_label_spaces() -> dict:\n",
        "    tr_p, tr_pl, tr_di = collect_split_mt(\"train\")\n",
        "    va_p, va_pl, va_di = collect_split_mt(\"validation\")\n",
        "    te_p, te_pl, te_di = collect_split_mt(\"test\")\n",
        "\n",
        "    plant_classes   = sorted(set(tr_pl + va_pl + te_pl))\n",
        "    disease_classes = sorted(set(tr_di + va_di + te_di))\n",
        "    p2i = {c:i for i,c in enumerate(plant_classes)}\n",
        "    d2i = {c:i for i,c in enumerate(disease_classes)}\n",
        "\n",
        "    tr_y_p = np.array([p2i[s] for s in tr_pl], dtype=np.int64)\n",
        "    tr_y_d = np.array([d2i[s] for s in tr_di], dtype=np.int64)\n",
        "    va_y_p = np.array([p2i[s] for s in va_pl], dtype=np.int64)\n",
        "    va_y_d = np.array([d2i[s] for s in va_di], dtype=np.int64)\n",
        "    te_y_p = np.array([p2i[s] for s in te_pl], dtype=np.int64)\n",
        "    te_y_d = np.array([d2i[s] for s in te_di], dtype=np.int64)\n",
        "\n",
        "    return dict(\n",
        "        train_paths=np.array(tr_p),  train_plant=tr_y_p,  train_disease=tr_y_d,\n",
        "        val_paths=np.array(va_p),    val_plant=va_y_p,    val_disease=va_y_d,\n",
        "        test_paths=np.array(te_p),   test_plant=te_y_p,   test_disease=te_y_d,\n",
        "        plant_classes=plant_classes, disease_classes=disease_classes\n",
        "    )\n",
        "\n",
        "class MTPathsDataset(Dataset):\n",
        "    \"\"\"Returns (image_tensor, plant_idx, disease_idx).\"\"\"\n",
        "    def __init__(self, paths, y_plant, y_disease, transform):\n",
        "        self.paths = paths; self.y_p = y_plant; self.y_d = y_disease\n",
        "        self.transform = transform\n",
        "    def __len__(self): return len(self.paths)\n",
        "    def __getitem__(self, i):\n",
        "        img = Image.open(self.paths[i]).convert(\"RGB\")\n",
        "        if self.transform is not None: img = self.transform(img)\n",
        "        return img, int(self.y_p[i]), int(self.y_d[i])\n",
        "\n",
        "base = build_label_spaces()\n",
        "print(f\"Plants: {len(base['plant_classes'])} | Diseases: {len(base['disease_classes'])}\")\n",
        "print(f\"Train={len(base['train_paths'])} | Val={len(base['val_paths'])} | Test={len(base['test_paths'])}\")\n",
        "\n",
        "# -----------------------\n",
        "# Subsample 20% of TRAIN if requested\n",
        "# -----------------------\n",
        "if SUBSAMPLE_FRAC < 1.0:\n",
        "    # stratified by PLANT (could also stratify jointly; plant is fine here)\n",
        "    by_c = {}\n",
        "    for i,t in enumerate(base['train_plant']): by_c.setdefault(int(t), []).append(i)\n",
        "    rng = np.random.default_rng(RANDOM_SEED)\n",
        "    idx_sub = []\n",
        "    for idxs in by_c.values():\n",
        "        k = max(1, int(round(len(idxs)*SUBSAMPLE_FRAC)))\n",
        "        idx_sub.extend(rng.choice(idxs, size=k, replace=False).tolist())\n",
        "    rng.shuffle(idx_sub)\n",
        "else:\n",
        "    idx_sub = np.arange(len(base['train_paths'])).tolist()\n",
        "\n",
        "train_ds_full = MTPathsDataset(base['train_paths'], base['train_plant'], base['train_disease'], transform=train_tfms)\n",
        "val_ds        = MTPathsDataset(base['val_paths'],   base['val_plant'],   base['val_disease'],   transform=eval_tfms)\n",
        "test_ds       = MTPathsDataset(base['test_paths'],  base['test_plant'],  base['test_disease'],  transform=eval_tfms)\n",
        "\n",
        "train_ds = Subset(train_ds_full, idx_sub)\n",
        "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=pin_mem)\n",
        "val_dl   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=pin_mem)\n",
        "test_dl  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=pin_mem)\n",
        "\n",
        "print(f\"Train subset: {len(train_ds)} (of {len(train_ds_full)})\")\n",
        "\n",
        "# -----------------------\n",
        "# Align weights helpers\n",
        "# -----------------------\n",
        "def align_weights_from_dict(classes: List[str], weights_dict: Optional[Dict[str, float]]):\n",
        "    \"\"\"Map user {class_name: weight} -> tensor aligned to classes order.\"\"\"\n",
        "    if not weights_dict:\n",
        "        return None\n",
        "    miss = [c for c in classes if c not in weights_dict]\n",
        "    extra = [k for k in weights_dict if k not in classes]\n",
        "    if miss:  warnings.warn(f\"[class weights] missing: {miss}; using 1.0\")\n",
        "    if extra: warnings.warn(f\"[class weights] extra keys ignored: {extra}\")\n",
        "    arr = np.array([weights_dict.get(c, 1.0) for c in classes], dtype=np.float32)\n",
        "    return torch.tensor(arr, device=device)\n",
        "\n",
        "def balanced_weights_from_subset(targets: np.ndarray, num_classes: int) -> torch.Tensor:\n",
        "    \"\"\"Classic balanced weights computed from subset labels.\"\"\"\n",
        "    counts = np.bincount(targets, minlength=num_classes).astype(np.float32)\n",
        "    w = (counts.sum() / (num_classes * np.clip(counts, 1, None))).astype(np.float32)\n",
        "    return torch.tensor(w, device=device)\n",
        "\n",
        "# Build per-head CE weights\n",
        "sub_idx_np = np.array(idx_sub, dtype=np.int64)\n",
        "train_pl_subset = base['train_plant'][sub_idx_np]\n",
        "train_di_subset = base['train_disease'][sub_idx_np]\n",
        "\n",
        "w_pl_user = align_weights_from_dict(base['plant_classes'],   class_weights_plants)\n",
        "w_di_user = align_weights_from_dict(base['disease_classes'], class_weights_diseases)\n",
        "\n",
        "w_pl_bal  = balanced_weights_from_subset(train_pl_subset, len(base['plant_classes']))\n",
        "w_di_bal  = balanced_weights_from_subset(train_di_subset, len(base['disease_classes']))\n",
        "\n",
        "w_pl = w_pl_user if w_pl_user is not None else w_pl_bal\n",
        "w_di = w_di_user if w_di_user is not None else w_di_bal\n",
        "\n",
        "# -----------------------\n",
        "# Houdini loss (fallback if not provided globally)\n",
        "# -----------------------\n",
        "class HoudiniMarginLoss(nn.Module):\n",
        "    \"\"\"Safe Houdini-like margin loss.\"\"\"\n",
        "    def __init__(self, tau: float = 1.0):\n",
        "        super().__init__()\n",
        "        self.tau = tau\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, logits, y):\n",
        "        s_true = logits.gather(1, y.view(-1,1))\n",
        "        mask = torch.ones_like(logits, dtype=torch.bool)\n",
        "        mask.scatter_(1, y.view(-1,1), False)\n",
        "        s_oth = logits.masked_fill(~mask, float('-inf')).amax(dim=1, keepdim=True)\n",
        "        margin = (s_oth - s_true) / self.tau\n",
        "        return self.sigmoid(margin).mean()\n",
        "\n",
        "USE_HOUDINI = ('houdini_loss' in globals())\n",
        "if not USE_HOUDINI:\n",
        "    houdini_loss = HoudiniMarginLoss(tau=1.0)  # local fallback\n",
        "print(\"Using\", \"external houdini_loss.\" if USE_HOUDINI else \"local HoudiniMarginLoss(tau=1.0).\")\n",
        "\n",
        "# -----------------------\n",
        "# Model: DeiT backbone + two task heads\n",
        "# -----------------------\n",
        "# num_classes=0 returns features; set drop_path_rate for stochastic depth regularization\n",
        "backbone = timm.create_model('deit_small_patch16_224', pretrained=True, num_classes=0, drop_path_rate=DROPPATH_RATE)\n",
        "backbone.to(device).train()\n",
        "\n",
        "# infer feature dimension\n",
        "with torch.no_grad():\n",
        "    dummy = torch.zeros(1,3,IMG_SIZE,IMG_SIZE).to(device)\n",
        "    feat = backbone(dummy)  # shape [B, D]\n",
        "feat_dim = feat.shape[-1]\n",
        "\n",
        "plant_head = nn.Sequential(\n",
        "    nn.Dropout(HEAD_DROPOUT),\n",
        "    nn.Linear(feat_dim, len(base['plant_classes']))\n",
        ").to(device)\n",
        "\n",
        "disease_head = nn.Sequential(\n",
        "    nn.Dropout(HEAD_DROPOUT),\n",
        "    nn.Linear(feat_dim, len(base['disease_classes']))\n",
        ").to(device)\n",
        "\n",
        "params_M = (sum(p.numel() for p in backbone.parameters())\n",
        "            + sum(p.numel() for p in plant_head.parameters())\n",
        "            + sum(p.numel() for p in disease_head.parameters()))/1e6\n",
        "\n",
        "# -----------------------\n",
        "# Criterion: mixed per head  (0.3 * Houdini + 0.7 * CE(weights, label_smoothing))\n",
        "# -----------------------\n",
        "ce_pl = nn.CrossEntropyLoss(weight=w_pl, label_smoothing=LABEL_SMOOTHING)\n",
        "ce_di = nn.CrossEntropyLoss(weight=w_di, label_smoothing=LABEL_SMOOTHING)\n",
        "\n",
        "def mixed_loss(logits_pl, y_pl, logits_di, y_di):\n",
        "    # Houdini terms\n",
        "    h_pl = houdini_loss(logits_pl, y_pl)\n",
        "    h_di = houdini_loss(logits_di, y_di)\n",
        "    # CE terms\n",
        "    ce_p = ce_pl(logits_pl, y_pl)\n",
        "    ce_d = ce_di(logits_di, y_di)\n",
        "    # mix per head, then average across heads\n",
        "    loss_pl = 0.3 * h_pl + 0.7 * ce_p\n",
        "    loss_di = 0.3 * h_di + 0.7 * ce_d\n",
        "    return 0.5 * (loss_pl + loss_di), (h_pl.item(), ce_p.item(), h_di.item(), ce_d.item())\n",
        "\n",
        "# -----------------------\n",
        "# Optimizer / Scheduler / AMP\n",
        "# -----------------------\n",
        "all_params = list(backbone.parameters()) + list(plant_head.parameters()) + list(disease_head.parameters())\n",
        "optimizer = torch.optim.AdamW(all_params, lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts( optimizer, T_0=3, T_mult=2, eta_min=1e-6)\n",
        " # T_0  number of epoch for first cicle/ T_mult=x its mean xT_i/ eta_min min Learning Rate\n",
        "scaler    = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n",
        "\n",
        "# -----------------------\n",
        "# Eval helpers (per-head metrics + combined macro-F1)\n",
        "# -----------------------\n",
        "@torch.no_grad()\n",
        "def eval_split(dl, name=\"split\"):\n",
        "    backbone.eval(); plant_head.eval(); disease_head.eval()\n",
        "    tot = 0; loss_sum = 0.0\n",
        "    ytp=[]; ypp=[]; ytd=[]; ypd=[]\n",
        "    for x, yp, yd in dl:\n",
        "        x, yp, yd = x.to(device), yp.to(device), yd.to(device)\n",
        "        with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
        "            feats = backbone(x)\n",
        "            lp = plant_head(feats)\n",
        "            ld = disease_head(feats)\n",
        "            loss, _ = mixed_loss(lp, yp, ld, yd)\n",
        "        loss_sum += loss.item() * x.size(0)\n",
        "        tot += x.size(0)\n",
        "        ytp.append(yp.cpu().numpy()); ypp.append(lp.argmax(1).cpu().numpy())\n",
        "        ytd.append(yd.cpu().numpy()); ypd.append(ld.argmax(1).cpu().numpy())\n",
        "    if tot == 0:\n",
        "        return dict(loss=0.0, acc_p=0.0, f1m_p=0.0, acc_d=0.0, f1m_d=0.0, f1m_avg=0.0)\n",
        "    ytp = np.concatenate(ytp); ypp = np.concatenate(ypp)\n",
        "    ytd = np.concatenate(ytd); ypd = np.concatenate(ypd)\n",
        "    acc_p = accuracy_score(ytp, ypp); f1m_p = f1_score(ytp, ypp, average='macro', zero_division=0)\n",
        "    acc_d = accuracy_score(ytd, ypd); f1m_d = f1_score(ytd, ypd, average='macro', zero_division=0)\n",
        "    return dict(loss=loss_sum/tot, acc_p=acc_p, f1m_p=f1m_p, acc_d=acc_d, f1m_d=f1m_d, f1m_avg=(f1m_p+f1m_d)/2)\n",
        "\n",
        "# -----------------------\n",
        "# Train one epoch\n",
        "# -----------------------\n",
        "def train_one_epoch():\n",
        "    backbone.train(); plant_head.train(); disease_head.train()\n",
        "    tot = 0; loss_sum = 0.0\n",
        "    hpl_sum=0.0; cepl_sum=0.0; hdi_sum=0.0; cedi_sum=0.0\n",
        "    corr_p=0; corr_d=0\n",
        "    for x, yp, yd in train_dl:\n",
        "        x, yp, yd = x.to(device), yp.to(device), yd.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
        "            feats = backbone(x)\n",
        "            lp = plant_head(feats)\n",
        "            ld = disease_head(feats)\n",
        "            loss, (hpl, cepl, hdi, cedi) = mixed_loss(lp, yp, ld, yd)\n",
        "        scaler.scale(loss).backward()\n",
        "        if MAX_GRAD_NORM is not None:\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(all_params, MAX_GRAD_NORM)\n",
        "        scaler.step(optimizer); scaler.update()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            bs = x.size(0); tot += bs\n",
        "            loss_sum += loss.item()*bs\n",
        "            hpl_sum += hpl*bs; cepl_sum += cepl*bs\n",
        "            hdi_sum += hdi*bs; cedi_sum += cedi*bs\n",
        "            corr_p += (lp.argmax(1)==yp).sum().item()\n",
        "            corr_d += (ld.argmax(1)==yd).sum().item()\n",
        "\n",
        "    return dict(\n",
        "        loss=loss_sum/max(1,tot),\n",
        "        houdini_p=hpl_sum/max(1,tot),\n",
        "        ce_p=cepl_sum/max(1,tot),\n",
        "        houdini_d=hdi_sum/max(1,tot),\n",
        "        ce_d=cedi_sum/max(1,tot),\n",
        "        acc_p=corr_p/max(1,tot),\n",
        "        acc_d=corr_d/max(1,tot),\n",
        "    )\n",
        "\n",
        "# -----------------------\n",
        "# Train loop with early stopping on avg(val macro-F1)\n",
        "# -----------------------\n",
        "best_score, wait = -1.0, 0\n",
        "best_state = None\n",
        "history = []\n",
        "\n",
        "train_wall_start = time.time()\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    t0 = time.time()\n",
        "    # train\n",
        "    t_train0 = time.time()\n",
        "    tr = train_one_epoch()\n",
        "    train_secs = time.time() - t_train0\n",
        "\n",
        "    # val\n",
        "    va = eval_split(val_dl, name=\"val\")\n",
        "\n",
        "    # optional test\n",
        "    if TEST_EVERY_EPOCH and test_dl is not None:\n",
        "        te = eval_split(test_dl, name=\"test\")\n",
        "    else:\n",
        "        te = dict(loss=0.0, acc_p=0.0, f1m_p=0.0, acc_d=0.0, f1m_d=0.0, f1m_avg=0.0)\n",
        "\n",
        "    scheduler.step()\n",
        "    epoch_secs = time.time() - t0\n",
        "    denom = len(train_ds)\n",
        "    train_ips = denom / max(1e-9, train_secs)\n",
        "\n",
        "    history.append({\n",
        "        'epoch': epoch,\n",
        "        'train': tr,\n",
        "        'val': va,\n",
        "        'test': te,\n",
        "        'train_secs': float(train_secs),\n",
        "        'epoch_secs': float(epoch_secs),\n",
        "        'train_ips':  float(train_ips),\n",
        "    })\n",
        "\n",
        "    print(f\"epoch {epoch:02d} | \"\n",
        "          f\"train loss {tr['loss']:.3f} accP {tr['acc_p']:.3f} accD {tr['acc_d']:.3f} \"\n",
        "          f\"({train_secs:.1f}s, {train_ips:.1f} img/s) | \"\n",
        "          f\"val f1P {va['f1m_p']:.3f} f1D {va['f1m_d']:.3f} avg {va['f1m_avg']:.3f} loss {va['loss']:.3f} | \"\n",
        "          f\"test f1P {te['f1m_p']:.3f} f1D {te['f1m_d']:.3f} avg {te['f1m_avg']:.3f} | \"\n",
        "          f\"epoch {epoch_secs:.1f}s\")\n",
        "\n",
        "    # early stopping on average macro-F1 across the two heads\n",
        "    score = va['f1m_avg']\n",
        "    if score > best_score:\n",
        "        best_score, wait = float(score), 0\n",
        "        best_state = {\n",
        "            'backbone': {k: v.detach().cpu().clone() for k,v in backbone.state_dict().items()},\n",
        "            'plant_head': {k: v.detach().cpu().clone() for k,v in plant_head.state_dict().items()},\n",
        "            'disease_head': {k: v.detach().cpu().clone() for k,v in disease_head.state_dict().items()},\n",
        "        }\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= PATIENCE:\n",
        "            print(\"Early stopping.\")\n",
        "            break\n",
        "\n",
        "total_train_secs = time.time() - train_wall_start\n",
        "\n",
        "# -----------------------\n",
        "# Final eval with best weights + reports\n",
        "# -----------------------\n",
        "if best_state is not None:\n",
        "    backbone.load_state_dict(best_state['backbone'], strict=True)\n",
        "    plant_head.load_state_dict(best_state['plant_head'], strict=True)\n",
        "    disease_head.load_state_dict(best_state['disease_head'], strict=True)\n",
        "    backbone.to(device).eval(); plant_head.to(device).eval(); disease_head.to(device).eval()\n",
        "\n",
        "    va = eval_split(val_dl,  name=\"val(final)\")\n",
        "    te = eval_split(test_dl, name=\"test(final)\") if test_dl is not None else dict(loss=0, acc_p=0, f1m_p=0, acc_d=0, f1m_d=0, f1m_avg=0)\n",
        "\n",
        "    print(\"Best weights restored from memory. Best val avg F1-macro =\", round(best_score, 4))\n",
        "    print(\"Final VAL:  accP={:.4f} f1P={:.4f} accD={:.4f} f1D={:.4f} avgF1={:.4f} loss={:.4f}\".format(\n",
        "        va['acc_p'], va['f1m_p'], va['acc_d'], va['f1m_d'], va['f1m_avg'], va['loss']))\n",
        "    print(\"Final TEST: accP={:.4f} f1P={:.4f} accD={:.4f} f1D={:.4f} avgF1={:.4f} loss={:.4f}\".format(\n",
        "        te['acc_p'], te['f1m_p'], te['acc_d'], te['f1m_d'], te['f1m_avg'], te['loss']))\n",
        "\n",
        "    # Detailed per-head classification reports on TEST\n",
        "    @torch.no_grad()\n",
        "    def test_reports():\n",
        "        if test_dl is None:\n",
        "            print(\"TEST split not found; skipping reports.\"); return\n",
        "        ytp=[]; ypp=[]; ytd=[]; ypd=[]\n",
        "        for x, yp, yd in test_dl:\n",
        "            x = x.to(device)\n",
        "            feats = backbone(x)\n",
        "            lp = plant_head(feats); ld = disease_head(feats)\n",
        "            ypp.append(lp.argmax(1).cpu().numpy()); ytp.append(yp.numpy())\n",
        "            ypd.append(ld.argmax(1).cpu().numpy()); ytd.append(yd.numpy())\n",
        "        ytp=np.concatenate(ytp); ypp=np.concatenate(ypp)\n",
        "        ytd=np.concatenate(ytd); ypd=np.concatenate(ypd)\n",
        "        print(\"\\n[TEST report — PLANT]\")\n",
        "        print(classification_report(ytp, ypp, target_names=base['plant_classes'], digits=3, zero_division=0))\n",
        "        print(\"\\n[TEST report — DISEASE]\")\n",
        "        print(classification_report(ytd, ypd, target_names=base['disease_classes'], digits=3, zero_division=0))\n",
        "    test_reports()\n",
        "\n",
        "    output_summary = {\n",
        "        'name': MODEL_NAME,\n",
        "        'params_M': float(params_M),\n",
        "        'best_val_avg_f1_macro': float(best_score),\n",
        "        'final_val': va,\n",
        "        'final_test': te,\n",
        "        'total_train_secs': float(total_train_secs),\n",
        "        'avg_epoch_secs': float(np.mean([h['epoch_secs'] for h in history])) if history else float('nan'),\n",
        "    }\n",
        "    output = {'history': history, 'summary': output_summary}\n",
        "    print(output_summary)\n",
        "else:\n",
        "    print(\"Warning: No best_state stored in memory. Skipping final evaluation.\")\n",
        "    output_summary = {\n",
        "        'name': MODEL_NAME,\n",
        "        'params_M': float(params_M),\n",
        "        'best_val_avg_f1_macro': float('nan'),\n",
        "        'final_val': None,\n",
        "        'final_test': None,\n",
        "        'total_train_secs': float(total_train_secs),\n",
        "        'avg_epoch_secs': float(np.mean([h['epoch_secs'] for h in history])) if history else float('nan'),\n",
        "    }\n",
        "    output = {'history': history, 'summary': output_summary}\n",
        "    print(output_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SpwZtJyai81"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Plots & Clusters for DeiT Multi-Task Training\n",
        "# - Works with the history/output of the DeiT multi-task script\n",
        "# - Draws curves for plant/disease heads and averaged F1\n",
        "# - Builds 2D clusters of features (PCA->t-SNE), colored by plant/disease\n",
        "# =========================\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# -------- utils to read history safely --------\n",
        "def _get(hist, key_path, default=np.nan):\n",
        "    \"\"\"Safely dig nested metrics: key_path = ('val','f1m_p'), etc.\"\"\"\n",
        "    cur = hist\n",
        "    for k in key_path:\n",
        "        if cur is None: return default\n",
        "        cur = cur.get(k, None)\n",
        "    return cur if cur is not None else default\n",
        "\n",
        "def is_multitask_history(sample_epoch_dict):\n",
        "    \"\"\"Detect multi-task history by presence of plant/disease metrics.\"\"\"\n",
        "    if not sample_epoch_dict: return False\n",
        "    vt = sample_epoch_dict.get('val', {})\n",
        "    # multi-task history contains f1m_p / f1m_d (or acc_p / acc_d in train)\n",
        "    return ('f1m_p' in vt and 'f1m_d' in vt) or ('acc_p' in sample_epoch_dict.get('train', {}))\n",
        "\n",
        "# -------- history adapter: supports both single- and multi-task --------\n",
        "hist = output['history'] if isinstance(output, dict) and 'history' in output else output\n",
        "assert isinstance(hist, list) and len(hist) > 0, \"History is empty or invalid.\"\n",
        "\n",
        "epochs = [d['epoch'] for d in hist]\n",
        "mt = is_multitask_history(hist[0])\n",
        "\n",
        "# =========================\n",
        "# CURVES\n",
        "# =========================\n",
        "\n",
        "if mt:\n",
        "    # -------- Accuracy (train) per head --------\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, [d['train'].get('acc_p', np.nan) for d in hist], label=\"train acc (plant)\")\n",
        "    plt.plot(epochs, [d['train'].get('acc_d', np.nan) for d in hist], label=\"train acc (disease)\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.legend(); plt.title(\"Train Accuracy (Plant/Disease)\")\n",
        "    plt.show()\n",
        "\n",
        "    # -------- Validation/Test F1 per head + average --------\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, [d['val'].get('f1m_p', np.nan) for d in hist],   label=\"val F1-macro (plant)\")\n",
        "    plt.plot(epochs, [d['val'].get('f1m_d', np.nan) for d in hist],   label=\"val F1-macro (disease)\")\n",
        "    plt.plot(epochs, [d['val'].get('f1m_avg', np.nan) for d in hist], label=\"val F1-macro (avg)\")\n",
        "    plt.plot(epochs, [d['test'].get('f1m_p', np.nan) for d in hist],  '--', label=\"test F1-macro (plant)\")\n",
        "    plt.plot(epochs, [d['test'].get('f1m_d', np.nan) for d in hist],  '--', label=\"test F1-macro (disease)\")\n",
        "    plt.plot(epochs, [d['test'].get('f1m_avg', np.nan) for d in hist],'--', label=\"test F1-macro (avg)\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"F1-macro\"); plt.legend(); plt.title(\"F1-macro (Plant/Disease/Average)\")\n",
        "    plt.show()\n",
        "\n",
        "    # -------- Loss (val/test) --------\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, [d['val'].get('loss', np.nan) for d in hist],   label=\"val loss\")\n",
        "    plt.plot(epochs, [d['test'].get('loss', np.nan) for d in hist],  label=\"test loss\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.title(\"Loss (Validation/Test)\")\n",
        "    plt.show()\n",
        "\n",
        "    # -------- Throughput & epoch time --------\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, [d.get('train_ips', np.nan) for d in hist])\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Images/sec (train)\"); plt.title(\"Training Throughput\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, [d.get('epoch_secs', np.nan) for d in hist])\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Seconds\"); plt.title(\"Epoch Wall-Clock Time\")\n",
        "    plt.show()\n",
        "\n",
        "    # -------- Optional: show loss components if you logged them --------\n",
        "    if 'houdini_p' in hist[0]['train']:\n",
        "        plt.figure()\n",
        "        plt.plot(epochs, [d['train'].get('houdini_p', np.nan) for d in hist], label=\"Houdini plant\")\n",
        "        plt.plot(epochs, [d['train'].get('ce_p', np.nan)      for d in hist], label=\"CE plant\")\n",
        "        plt.plot(epochs, [d['train'].get('houdini_d', np.nan) for d in hist], label=\"Houdini disease\")\n",
        "        plt.plot(epochs, [d['train'].get('ce_d', np.nan)      for d in hist], label=\"CE disease\")\n",
        "        plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss terms\"); plt.legend(); plt.title(\"Loss Components (Train)\")\n",
        "        plt.show()\n",
        "\n",
        "else:\n",
        "    # ====== Backwards-compatible (old single-task history) ======\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, [d['train']['acc'] for d in hist], label=\"train acc\")\n",
        "    plt.plot(epochs, [d['val']['acc']   for d in hist], label=\"val acc\")\n",
        "    plt.plot(epochs, [d['test']['acc']  for d in hist], label=\"test acc\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.legend(); plt.title(\"Accuracy\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, [d['train']['loss'] for d in hist], label=\"train loss\")\n",
        "    plt.plot(epochs, [d['val']['loss']   for d in hist], label=\"val loss\")\n",
        "    plt.plot(epochs, [d['test']['loss']  for d in hist], label=\"test loss\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.title(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, [d['val']['f1_macro']  for d in hist], label=\"val f1_macro\")\n",
        "    plt.plot(epochs, [d['test']['f1_macro'] for d in hist], label=\"test f1_macro\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"F1-macro\"); plt.legend(); plt.title(\"F1-macro\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, [d['val']['f1_weighted']  for d in hist], label=\"val f1_weighted\")\n",
        "    plt.plot(epochs, [d['test']['f1_weighted'] for d in hist], label=\"test f1_weighted\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"F1-weighted\"); plt.legend(); plt.title(\"F1-weighted\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, [d['train_ips'] for d in hist])\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Images/sec (train)\"); plt.title(\"Training Throughput\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, [d['epoch_secs'] for d in hist])\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Seconds\"); plt.title(\"Epoch Wall-Clock Time\")\n",
        "    plt.show()\n",
        "\n",
        "# =========================\n",
        "# CLUSTER PLOTS (PCA -> t-SNE) for features\n",
        "# - Requires: backbone, test_dl, device, and the class lists in `base`\n",
        "# - Colors by plant classes and by disease classes\n",
        "# =========================\n",
        "@torch.no_grad()\n",
        "def collect_features_and_labels(backbone, dl, max_samples=4000):\n",
        "    \"\"\"Collect backbone features and both labels from a dataloader.\"\"\"\n",
        "    backbone.eval()\n",
        "    feats = []; y_pl = []; y_di = []\n",
        "    n_seen = 0\n",
        "    for x, yp, yd in dl:\n",
        "        x = x.to(device)\n",
        "        f = backbone(x)  # [B, D]\n",
        "        feats.append(f.detach().cpu().numpy())\n",
        "        y_pl.append(yp.numpy()); y_di.append(yd.numpy())\n",
        "        n_seen += x.size(0)\n",
        "        if n_seen >= max_samples:\n",
        "            break\n",
        "    F = np.concatenate(feats, axis=0)\n",
        "    YP = np.concatenate(y_pl, axis=0)\n",
        "    YD = np.concatenate(y_di, axis=0)\n",
        "    return F, YP, YD\n",
        "\n",
        "def tsne2d_from_feats(F, pca_dim=50, tsne_perplexity=35, tsne_iter=1000, random_state=42):\n",
        "    \"\"\"Light PCA compression then t-SNE to 2D.\"\"\"\n",
        "    if F.shape[1] > pca_dim:\n",
        "        Fp = PCA(n_components=pca_dim, random_state=random_state).fit_transform(F)\n",
        "    else:\n",
        "        Fp = F\n",
        "    T = TSNE(n_components=2, perplexity=tsne_perplexity, n_iter=tsne_iter, init='pca',\n",
        "             learning_rate='auto', random_state=random_state).fit_transform(Fp)\n",
        "    return T\n",
        "\n",
        "def scatter_clusters(T2, labels, title, class_names, alpha=0.7, s=10):\n",
        "    \"\"\"2D scatter with class colors (matplotlib default cycle).\"\"\"\n",
        "    plt.figure(figsize=(7,6))\n",
        "    # Draw per class to get a legend\n",
        "    uniq = np.unique(labels)\n",
        "    for c in uniq:\n",
        "        m = labels == c\n",
        "        plt.scatter(T2[m,0], T2[m,1], s=s, alpha=alpha, label=class_names[c])\n",
        "    plt.legend(markerscale=2, frameon=True, fontsize=8)\n",
        "    plt.title(title); plt.xlabel(\"t-SNE 1\"); plt.ylabel(\"t-SNE 2\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "try:\n",
        "    # 1) collect features from TEST split\n",
        "    F, YP, YD = collect_features_and_labels(backbone, test_dl, max_samples=4000)\n",
        "    # 2) tsne\n",
        "    T2 = tsne2d_from_feats(F, pca_dim=50, tsne_perplexity=35, tsne_iter=1200, random_state=RANDOM_SEED)\n",
        "    # 3) plot clusters by PLANT\n",
        "    scatter_clusters(T2, YP, title=\"Feature clusters — PLANT\", class_names=base['plant_classes'])\n",
        "    # 4) plot clusters by DISEASE\n",
        "    scatter_clusters(T2, YD, title=\"Feature clusters — DISEASE\", class_names=base['disease_classes'])\n",
        "except Exception as e:\n",
        "    print(f\"[cluster plots] skipped due to: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAmHldARQ-H3"
      },
      "source": [
        "Two head model (plant, deasese)\n",
        "Close to ideal Clasification of plants (easy task).\n",
        "Well clasification of disiase eith acurase = , vall = , good sine that we dont have owerfit. Only few clusters cross each other. F1 = , its mean preaty well bulens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSeWiQCUctbK"
      },
      "outputs": [],
